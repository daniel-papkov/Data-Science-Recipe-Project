{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What qualities contribute to a good recipe?\n",
    "Initially, we must import the necessary libraries to commence the scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from statistics import LinearRegression\n",
    "import time\n",
    "\n",
    "from numpy import average, shape\n",
    "\n",
    "from matplotlib.widgets import Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics  import r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping ####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we delve into the scraping process, our first step is to retrieve all the recipe names and corresponding links, saving them to a file for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_page_thespruceeats():#this returns a list of all the links to receipies on the page:\n",
    "    # URL to scrape\n",
    "    url = \"https://www.thespruceeats.com/search?q=&searchType=recipe\"\n",
    "\n",
    "    # Configure the Selenium webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run in headless mode (no GUI)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Get the page source and parse it with BeautifulSoup\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    results_div = soup.find(\"div\", attrs={\"class\": \"results-list__container\"})\n",
    "    recipe_names = []\n",
    "    recipe_links = []\n",
    "\n",
    "    # Scrape the first page\n",
    "    for li in results_div.find_all(\"li\", class_=\"results__item\"):\n",
    "        if li.find(\"a\") is not None:\n",
    "            link = li.find(\"a\").get(\"href\")\n",
    "        else:\n",
    "            link = ''\n",
    "\n",
    "        if li.find(\"h4\", class_=\"card__title\") is not None:\n",
    "            name = li.find(\"h4\", class_=\"card__title\").text.strip()\n",
    "        else:\n",
    "            name = ''\n",
    "        \n",
    "        recipe_names.append(name)\n",
    "        recipe_links.append(link)\n",
    "\n",
    "    # Scrape subsequent pages if the \"Next\" button exists\n",
    "    while True:\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".pagination__item-link--next\")))\n",
    "            next_button.click()\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Get the page source and parse it with BeautifulSoup\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "            results_div = soup.find(\"div\", attrs={\"class\": \"results-list__container\"})\n",
    "\n",
    "            # Scrape recipe names and links\n",
    "            for li in results_div.find_all(\"li\", class_=\"results__item\"):\n",
    "                if li.find(\"a\") is not None:\n",
    "                    link = li.find(\"a\").get(\"href\")\n",
    "                else:\n",
    "                    link = ''\n",
    "\n",
    "                if li.find(\"h4\", class_=\"card__title\") is not None:\n",
    "                    name = li.find(\"h4\", class_=\"card__title\").text.strip()\n",
    "                else:\n",
    "                    name = ''\n",
    "                print(f\"{name} added\")\n",
    "                recipe_names.append(name)\n",
    "                recipe_links.append(link)\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame with the recipe names and links\n",
    "    df = pd.DataFrame({\"Recipe_name\": recipe_names, \"Recipe_link\": recipe_links})\n",
    "\n",
    "    # Write DataFrame to a CSV file\n",
    "    df.to_csv(\"Recipe_Links_and_Names.csv\", index=False)\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute this function! By saving the retrieved data to a CSV file, we can expedite future work, reducing our dependency on the web driver and safeguarding against potential internet interruptions during subsequent tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pepper Steak Stir-Fry Recipe added\n",
      "Grilled Salmon Burgers With Radicchio Slaw and Sambal Mayonnaise added\n",
      "Kentucky Buck Cocktail Recipe added\n",
      "Sparkling Borage Cocktail added\n",
      "Lunch Box-Worthy Falafel Kebabs added\n",
      "Chipotle Pumpkin Queso Dip added\n",
      "Flamin' Hot Cheetos Mac and Cheese Bites added\n",
      "Cherry Vinegar Recipe added\n",
      "Turkish Ramadan Flat Bread (Pide) added\n",
      "Carrot, Cabbage, and Kohlrabi Slaw With Miso Dressing Recipe added\n",
      "Esquites Recipe (Mexican Corn Off the Cob) added\n",
      "Gillie Fix Cocktail Recipe added\n",
      "Raspberry Snakebite Recipe added\n",
      "Stuffed Italian Sourdough Loaf Recipe added\n",
      "Strawberry Chicken Salad With Champagne Vinaigrette Recipe added\n",
      "Lasagne All'astice (Lobster Lasagna) Recipe added\n",
      "Pear and Pomegranate Champagne Shrub Recipe added\n",
      "Homemade Smoked Maple Bacon added\n",
      "Baked S’mores Skillet Dip Recipe added\n",
      "Garlic Chicken Primavera Pasta added\n",
      "Cold Soba Noodle Salad Recipe added\n",
      "Vegetarian Tofu Tacos added\n",
      "Bay Hill Hummer added\n",
      "A Recipe for Risotto Made With Amarone Wine (Risotto all'Amarone) added\n",
      "Cranberry Orange Bread added\n",
      "Tropical Raspberry Smoothie Recipe added\n",
      "Mexican Lasagna added\n",
      "Grilled Prosciutto Wrapped Pork Chops added\n",
      "Cookie Dough Pops added\n",
      "Kentucky Coffee added\n",
      "Irish Ale Cocktail added\n",
      "Gluten-Free Fadge (Irish Potato Rolls) added\n",
      "Spring Greens: Beans With Lemon Ginger Butter Recipe added\n",
      "Super-Easy Balsamic Roasted Strawberry added\n",
      "Easy Classic Hollandaise Sauce With Tarragon added\n",
      "Easy Roasted Rhubarb Recipe added\n",
      "Traditional Smoked Mackerel Fishcakes added\n",
      "Pork and Onion Meatballs With Tomato Sauce added\n",
      "Easy and Traditional Blackberry Jelly added\n",
      "Yorkshire Ginger Parkin Biscuits added\n",
      "Easy Classic British Cheese Scones added\n",
      "Creamed Leeks and Smoked Haddock Recipe added\n",
      "Christmas Stollen Recipe added\n",
      "Anglesey Egg Recipe added\n",
      "White Bread (Made With Condensed Milk) Recipe added\n",
      "Garlic Chive Butter added\n",
      "Mint Butter Recipe added\n",
      "Easy Classic Elderflower Cordial added\n",
      "Welsh Leek and Stilton Soup added\n",
      "Super Apple and Almond Tart from Brendan Lynch added\n",
      "Venison Shepherd's Pie added\n",
      "Golden Beetroot Pasta added\n",
      "Gooseberry Compote added\n",
      "Bumbleberry Crumb Bars added\n",
      "Orange Sage Bread added\n",
      "World War I Era White Bread Recipe added\n",
      "Crispy Quinoa With Kale added\n",
      "Quinoa With Pasta Sauce and Cheese added\n",
      "Artichoke Gruyere Dip added\n",
      "The Recipe for Cheese Crouton added\n",
      "Peppermint Chocolate Spoons added\n",
      "Strawberry Pâte de Fruits Recipe added\n",
      "Tuna Ceviche Recipe added\n",
      "Matcha Green Tea Truffles added\n",
      "Keto Truffles added\n",
      "Chocolate-Dipped Plums added\n",
      "Red-White-and-Blue Cake Balls Recipe added\n",
      "Red, White, and Blue Gummies Recipe added\n",
      "Pumpkin Marshmallows added\n",
      "Basil Lemonade Recipe added\n",
      "Funfetti Lollipops added\n",
      "Chocolate Coconut Cream Patties added\n",
      "Tasty Nutella Cake Pops added\n",
      "Chocolate Firecrackers added\n",
      "Gluten-Free Chicken and Dumplings added\n",
      "Keto Angel Food Cake added\n",
      "Double Chocolate Chip Cookies Recipe added\n",
      "Keto Chocolate Cake Recipe added\n",
      "Reindeer Chow Recipe added\n",
      "Nutella Nougat added\n",
      "Meringue Rose Pops Recipe added\n",
      "Woodson’s Confidence “Cocktale” added\n",
      "Keto Queso Dip Recipe added\n",
      "Strawberries and Cream Cookies added\n",
      "Rose Jam added\n",
      "Sweet Potato Tacos added\n",
      "Kimchi Pickled Eggs Recipe added\n",
      "Tuna Tartare Recipe added\n",
      "Vegan Instant Pot Cashew Yogurt Recipe added\n",
      "Brown Butter Cookies added\n",
      "Pineapple Cheese Ball Recipe added\n",
      "Instant Pot Cauliflower added\n",
      "Sour Cream Coffee Cake Muffins added\n",
      "Keto Fat Bombs added\n",
      "Creamy Potato Salad added\n",
      "Eight Treasure Rice Pudding added\n",
      "Bubble Bread added\n",
      "Thai Grilled Pork Salad added\n",
      "Baked Turkey Wings Recipe added\n",
      "Snickerdoodle Cupcakes Recipe added\n",
      "Air Fryer Chicken Tenders Recipe added\n",
      "Italian Margarita Recipe added\n",
      "Thai Shrimp Salad (Shrimp Yum Goong) added\n",
      "Lyonnaise Salad added\n",
      "Tom Yum Fried Rice added\n",
      "Couscous Belboula (Barley Couscous) Recipe added\n",
      "Instant Pot Breakfast Casserole Recipe added\n",
      "Edible Cookie Dough added\n",
      "Crock Pot Sweet and Spicy Meatballs added\n",
      "Keto \"Apple\" Crisp Recipe added\n",
      "Three-Ingredient Swirled Nutella Fudge added\n",
      "Lavender Fudge added\n",
      "Keto Crab Cakes Recipe added\n",
      "Sugar-Free Chocolate Nut Clusters added\n",
      "Mango and Ginger Kombucha Mule With Tequila added\n",
      "Christmas Tree Pretzel Rods added\n",
      "Haupia Pie Recipe added\n",
      "Classic Butterscotch Drops Recipe added\n",
      "Bacon-Wrapped Jalapeño Poppers added\n",
      "Chili Verde Recipe added\n",
      "Mazurek Królewski: Polish Royal Mazurek Recipe added\n",
      "Roasted Carrots and Parsnips With Fresh Herbs Recipe added\n",
      "Czech Fried Cheese (Syr Smazeny) added\n",
      "Traditional Peach Butter added\n",
      "Blueberry-Filled Pierogi added\n",
      "Pierogi and Naleśniki Meat Filling Recipe added\n",
      "Plum Pierogi Filling added\n",
      "Fast Limoncello added\n",
      "Easy Rhubarb Compote added\n",
      "Elderflower Syrup added\n",
      "Potatoes Romanoff added\n",
      "Kentucky Bourbon Sauce Barbeque added\n",
      "Best Baked Beans Recipe added\n",
      "Creamy Corn Custard added\n",
      "Polish Sauerkraut Soup (Kapusniak) added\n",
      "Romanian Sausages (Mititei) added\n",
      "Romanian Pork Cordon Bleu Schnitzels Recipe added\n",
      "Polish Potato Salad (Sałatka Kartofli) added\n",
      "Serbian Coleslaw Recipe added\n",
      "Seared Scallops and Shrimp With Balsamic Strawberries added\n",
      "Classic French Steamed Mussels added\n",
      "Lemongrass Ginger Sauce added\n",
      "Classic Pan Sauce for Fish added\n",
      "Easy Classic Duck Confit Recipe added\n",
      "Gingerbread Cookies - Czech Pernik na Figurky added\n",
      "Quick Serbian Kajmak Recipe added\n",
      "Traditional Dutch Taai-Taai Cookie added\n",
      "Stroopkoeken (Dutch Caramel Cookies) Recipe added\n",
      "Pulled Pork Croquettes added\n",
      "Groninger Mustard Soup added\n",
      "Ukrainian Meatless Beet Soup (Borshch) added\n",
      "Bulgarian Fish Roe Appetizer - Tarama added\n",
      "Orange Mint Tea added\n",
      "Easy Almond Bundt Cake Recipe With Almond-Flavored Icing added\n",
      "Low-Sugar Blueberry-Rhubarb Jam Recipe added\n",
      "Polish Wafle Wafer Cookies added\n",
      "Polish Strawberry Kissel added\n",
      "Grilled Shrimp and Scallops Skewers (Uncle Mike's Spice Mix) added\n",
      "Rhubarb BBQ Sauce Recipe added\n",
      "Dairy Free Carrot Cake Recipe added\n",
      "Dairy-Free Gluten-Free Banana Bread for Celiacs added\n",
      "Dairy-Free White Cake added\n",
      "Vegan Buttercream Frosting added\n",
      "Vegan Buttercream Frosting Recipe added\n",
      "Seafood a la King With Tilapia, Shrimp, and Crab added\n",
      "Cajun Perch Po'Boys Recipe added\n",
      "Fish and Seafood, Beer Batter Style added\n",
      "Low-Sugar Spiced Plum Jam Recipe added\n",
      "Vegan Pumpkin Cupcakes added\n",
      "Vegan No Bake Cookies added\n",
      "Dairy-Free Beef Stroganoff added\n",
      "Vegetarian Yellow Split Pea Soup added\n",
      "Dairy-Free Basil Cashew Pesto: Savory Sauces added\n",
      "Homemade Cashew Butter for Vegans added\n",
      "Dairy-Free Chocolate Covered Pretzels added\n",
      "Dairy-Free Gingerbread People added\n",
      "Dairy-Free Vegan Whoopie Pies added\n",
      "Fresh Peach Topping for Ice Cream added\n",
      "Croatian Sour Cherry Strudel (Štrudlu s Višnjama) added\n",
      "Authentic Croatian Bajadera Torte Recipe added\n",
      "Polish Smoked Sausage and Sauerkraut added\n",
      "Polish Mushroom Pierogi and Naleśniki Filling Recipe added\n",
      "Polish Open-Faced Sandwich (Zapiekanka) added\n",
      "Day-Old Bread Gets New Life in German Bread Dumplings added\n",
      "German Mashed Apples and Potatoes added\n",
      "Gluten-Free Country Peach Cobbler Recipe added\n",
      "Homemade Gluten-Free Corn Tortilla added\n",
      "Gluten-Free Cheese Sauce added\n",
      "Galaktoboureko: Custard Pie With Phyllo added\n",
      "Easy 4-Ingredient Baked Brisket added\n",
      "Layered Strawberry Dessert With Ladyfingers and Cream added\n",
      "German Fresh Fruit Torte added\n",
      "Gluten-Free Apple Oat Muffins added\n",
      "Chocolate Raspberry Tart added\n",
      "Carrot Cake Ice Cream added\n",
      "Strawberries With Balsamic Black Pepper added\n",
      "Herbed Vegetable Soup added\n",
      "Gluten-Free Nacho Cheese Dip added\n",
      "Gluten-Free Apple Pie Recipe added\n",
      "Southern-Style Gluten-Free Banana Meringue Pudding added\n",
      "Gluten-Free Caramel Corn added\n",
      "Tsoureki (Greek Easter Bread) Recipe added\n",
      "Kolokithia Yemista Recipe (Papoutsakia-Style) added\n",
      "Succulent Glazes and Rubs for Cornish Game Hens added\n",
      "Roasted Cod With Tomatoes, Basil, and Mozzarella added\n",
      "Mexican Flag Shooter (Bandera Mexicana) added\n",
      "How To Make Miso Marinade added\n",
      "Pasta With Mixed Seafood (Pasta alla Posillipo) added\n",
      "Classic French Winter Vegetable Ragout added\n",
      "German Pound Cake with Lemon and Vanilla Recipe added\n",
      "German Cheesecake With Quark (Kaesekuchen) added\n",
      "German Mini Nut Bars (Nussecken) added\n",
      "Austrian Pancakes With Raisins (Kaiserschmarrn) added\n",
      "Make Your Own Refreshing Watermelon Martinis added\n",
      "Penne Pasta With Fresh Artichokes Recipe added\n",
      "Moshari Kokkinisto or Reddened Beef Stew Recipe added\n",
      "Ariani: A Refreshing Yogurt Beverage added\n",
      "Lamb Meatballs With Tzatziki Sauce added\n",
      "French Macaroni and Cheese Recipe added\n",
      "French Pork Medallions and Caramelized Apples added\n",
      "Classic French Canadian Venison Tourtiere added\n",
      "Traditional French Scallops Sage Cream added\n",
      "Black Forest Bread - Schwarzwaelder Kruste added\n",
      "German Lentil Stew With Noodles and Frankfurters added\n",
      "German Flädlesuppe added\n",
      "German Yogurt Salad Dressing added\n",
      "Greek Fisherman's Soup Recipe (Kakavia) added\n",
      "Psarosoupa (Fish Soup With Red Snapper and Vegetables) Recipe added\n",
      "French Crepes With Salted Butter Caramel added\n",
      "Jambon Beurre (Ham Butter Sandwich) Recipe added\n",
      "French Strawberry and Vanilla Charlotte Dessert added\n",
      "Pennsylvania Dutch Low-Sugar Apple Butter added\n",
      "German Osterbrot Easter Bread added\n",
      "Easy Baguette (Stangenbrot) Bread added\n",
      "Recipe for Rye Bread with Sourdough (Roggenbrot) added\n",
      "Whole Rye Bread Recipe added\n",
      "Streuseltaler German Pastry Recipe added\n",
      "Salmon With Citrus-Balsamic Vinaigrette added\n",
      "Brazilian Carrot Cake - Bolo de Cenoura Com Cobertura de Chocolate added\n",
      "Hallullas: Chilean \"Biscuits\" added\n",
      "Bien Me Sabe - Venezuelan Coconut Cream Cake added\n",
      "Bolo de Fuba (Brazilian Cornmeal Cake) Recipe added\n",
      "Easy Baked Ham With Pineapple added\n",
      "Crockpot Short Ribs and Rice added\n",
      "Apple Crisp With Blueberries and Raspberries added\n",
      "Classic Shortbread added\n",
      "Chicken and Ham Pasta Bake Recipe added\n",
      "Chicken and Orzo Bake added\n",
      "Chicken Salad With Walnuts and Grapes Recipe added\n",
      "Chicken and Rice Salad added\n",
      "Cranberry Pumpkin Seed Muffins added\n",
      "Seafood Casserole With Rice or Pasta added\n",
      "Slow Cooker Chicken With Honey-Hoisin Sauce Recipe added\n",
      "Slow Cooker Chicken and Shrimp With Fettuccine added\n",
      "Brown Sugar and Cinnamon Funnel Cakes added\n",
      "Shrimp and Crab Gumbo added\n",
      "Pepper and Garlic Eye of Round Roast Recipe added\n",
      "Roast Beef With Garlic and Thyme Recipe added\n",
      "Pancakes With Strawberry Sauce Recipe added\n",
      "Arroz al Horno, an Oven-Baked Spanish Rice Recipe added\n",
      "Bacalao con Tomate Recipe added\n",
      "Noodle Paella Recipe: Fideuà added\n",
      "Spanish Roast Lamb (Cordero Asado or Lechazo) added\n",
      "Ham, Cheese, and Chorizo Appetizer With Bread added\n",
      "Patatas Alioli: Alioli Potatoes added\n",
      "Spanish Crayfish in Tomato Sauce added\n",
      "Shrimp With Mushrooms, Garlic and Wine Tapa added\n",
      "Spanish Potatoes au Gratin (Patatas Gratinadas) added\n",
      "Make Fried Cauliflower...Spanish Style added\n",
      "Puré de Patatas con Ajo y Pimenton (Garlic Paprika Mashed Potatoes) added\n",
      "Turkey Bacon Ranch Sandwich added\n",
      "Thanksgiving Leftovers Party Buns added\n",
      "Chicken Po'boy Sandwich Recipe added\n",
      "Korean Barbecue Steamed Buns With Bulgogi Beef Recipe added\n",
      "Chorizo and Potato Empanadas added\n",
      "Southern Style Sweet Fruit Tea Three Ways added\n",
      "Cabbage Casserole With Cheddar Cheese added\n",
      "Skillet Buttermilk Cornbread With Corn added\n",
      "Cheese and Bacon Cornbread added\n",
      "Crock Pot Swiss Chicken Casserole added\n",
      "Eggplant and Tomato Casserole With Mozzarella Cheese added\n",
      "Green Tomato Cake With Nuts added\n",
      "Quick Ham and Noodle Casserole added\n",
      "Baked Mac and Cheese With Ham and Broccoli added\n",
      "Spinach and Beef Lasagna With Ricotta Cheese added\n",
      "English Pea Pesto Pasta With Ham added\n",
      "Vanilla Wafer Cookie Crumb Crust added\n",
      "Sweet Onion Salad Dressing added\n",
      "Quick Pesto Roasted Chicken Recipe added\n",
      "Round Steak and Gravy for Supper added\n",
      "Spanish Chicken in Orange Sauce added\n",
      "Spanish Eggs in Purgatory added\n",
      "Three Green Chile Meatloaf Recipe added\n",
      "Easy Poblano Pepper Relish added\n",
      "Slow Cooker Feijoada (Brazilian Black Bean Stew) Recipe added\n",
      "Torta de Fiambre: Urugayan Ham and Cheese Tart added\n",
      "Cazuela De Camarón: Shrimp in Plantain and Peanut Sauce added\n",
      "Milanesa de Carne Recipe added\n",
      "Chimichurri Chicken en Croute Recipe added\n",
      "Texas Caviar Black-Eyed Pea Dip With Jalapeño Peppers added\n",
      "Basic Black Eyed Peas added\n",
      "Fudgy Chocolate Chip Walnut Brownies Recipe added\n",
      "Crunchy Pistachio Brittle added\n",
      "Southern Chocolate Chess Pie added\n",
      "Chocolate Cream Pie With Whipped Cream added\n",
      "How to Make No-Bake Chocolate Cream Pie added\n",
      "Candy Cane Cookies added\n",
      "Blueberry Muffin Mix in a Jar Recipe added\n",
      "Sweet and Spicy Crock Pot Cocktail Franks added\n",
      "Crockpot Barbecue Beef Sandwiches added\n",
      "Slow Cooker Chili With Taco Seasoning added\n",
      "Sausage Bites in Sweet and Tangy Barbecue Sauce added\n",
      "Crockpot Kielbasa With Cabbage and Potatoes added\n",
      "Balsamic Marinated Chicken Breasts added\n",
      "Green Tomato Soup With Country Ham Recipe added\n",
      "Grilled Pork Chops With Balsamic Marinade added\n",
      "Louisiana Grillades and Grits added\n",
      "Whole Grilled Onions added\n",
      "Easy Meatloaf Sliders added\n",
      "Classic Two Egg Vanilla Pound Cake added\n",
      "4-Ingredient Crock-Pot Pork Barbecue added\n",
      "Pumpkin Oat Muffins With Dates Recipe added\n",
      "Pulled Pork With Peppers added\n",
      "Salsa Rosa: A Creamy Spanish Cocktail Sauce added\n",
      "Spanish Champiñones, Pimientos y Ajo (Mushrooms, Peppers and Garlic) added\n",
      "Sopa de Lentejas: Spanish Lentil Soup added\n",
      "Spanish Vegetable Soup (Menestra de Verduras) added\n",
      "Hamburger Soup With Black-Eyed Peas and Kale added\n",
      "Homemade Pumpernickel Bread added\n",
      "Easy Egg-Free Chocolate Mousse added\n",
      "Slow Cooker Red Beans With Ham and Rice added\n",
      "Crock Pot Red Beans and Rice With Ham added\n",
      "Beer-Battered Chicken Strips added\n",
      "Appetizer Hot Dogs in Barbecue Sauce added\n",
      "Old-Fashioned Chocolate Ice Cream Soda added\n",
      "Pecan Praline Ice Cream Recipe added\n",
      "Baked Pork Tenderloin added\n",
      "Holiday Potato Casserole added\n",
      "Smashed Red Potatoes With Garlic added\n",
      "Loaded Potato Casserole added\n",
      "Roasted Split Turkey Breast With Cajun Spices added\n",
      "Easy Chocolate Zucchini Cake Recipe added\n",
      "Zucchini Patties With Parmesan Cheese added\n",
      "Zucchini Casserole With Tomato Sauce and Bacon Recipe added\n",
      "How to Cook Cracked Hominy: Maíz Trillado added\n",
      "Brazilian Risoles (Chicken and Cheese Croquettes) added\n",
      "Stuffed Yuca Balls - Yuquitas Rellenas added\n",
      "Empanadas de Pizza added\n",
      "Golden Apricot Nut Bread added\n",
      "Apple Crunch Dessert With Cinnamon added\n",
      "Pineapple Delight Bars Recipe added\n",
      "Open-Faced Reuben Burgers added\n",
      "Classic Reuben Sandwich added\n",
      "Homemade Crock Pot Barbecue Meatballs added\n",
      "Cocktail Meatballs With Apricot Preserves added\n",
      "Slow Cooker Honey Glazed Ham added\n",
      "Moist Banana Cupcakes With Vanilla Icing added\n",
      "Deviled Eggs With Mayonnaise Recipe added\n",
      "Kentucky Burgoo added\n",
      "Slow Cooker Lamb Chops With Tomatoes and Garlic added\n"
     ]
    }
   ],
   "source": [
    "get_full_page_thespruceeats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained the recipe links, our next step is to scrape each individual recipe. To streamline the process, we will break it down into manageable steps. The first step involves creating a \"soup\" object, which will allow us to parse the HTML content of each recipe page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Recipe_Links_and_Names.csv\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_soup_object(url):\n",
    "    ###\n",
    "    #url = \"https://www.thespruceeats.com/search?q=&searchType=recipe\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    return soup\n",
    "    ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will proceed to gather information in the order it is presented on our site. To ensure efficiency, we will encapsulate the gathering process within functions as we will be looping over them shortly. Our initial function will collect details such as cook time, prep time, total cooking time, and the number of servings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting this information can be challenging due to the varying formatting across different recipes. Time values can take on various forms, such as \"25 min,\" \"2h 15 min,\" \"2 hours 10 min,\" or numerous other variations. To address this issue, we have implemented a complex regular expression (regex) expression to handle the diverse time formats encountered during scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cook_times(soup_obj):\n",
    "    lines=[]\n",
    "    results_items = soup_obj\n",
    "    results_items = soup_obj.find_all(class_='comp article__decision-block mntl-block')\n",
    "    if(results_items==[]):\n",
    "        soup = soup_obj\n",
    "        results_items = soup.find_all(class_='comp project-meta')        \n",
    "\n",
    "    for item in results_items:\n",
    "        item.find_all(class_='meta-text__data')\n",
    "        for sub_item in item:\n",
    "            if bool(sub_item.text.strip()):\n",
    "                clean_text = sub_item.text.strip().replace('\\n', '')\n",
    "                lines.append(clean_text)\n",
    "\n",
    "    if(len(lines)>1):\n",
    "        new_string = lines[0] + lines[1]\n",
    "        lines[0]= new_string\n",
    "\n",
    "    #use regex expressions to clean up the line we get it looks something like this\n",
    "    #['Prep: 15 minsCook: 20 minsTotal: 35 minsServings: 6 servingsYield: 1 cake', 'ratingsAdd a comment']\n",
    "    #prep = re.findall(r'Prep:\\s*(\\d+)\\s*mins', lines[0])[0]\n",
    "    cook_time_str = re.findall(r'Cook:\\s*(?:(\\d+)\\s*(?:hrs?|hours?)\\s*)?(?:(\\d+)\\s*mins?)?', lines[0])[0]\n",
    "    prep_time_str = re.findall(r'Prep:\\s*(?:(\\d+)\\s*(?:hrs?|hours?)\\s*)?(?:(\\d+)\\s*mins?)?', lines[0])[0]    \n",
    "    total_time_str = re.findall(r'Total:\\s*(?:(\\d+)\\s*(?:hrs?|hours?)\\s*)?(?:(\\d+)\\s*mins?)?', lines[0])[0]\n",
    "# Convert cook time to minutes\n",
    "\n",
    "\n",
    "    hours = int(cook_time_str[0]) if cook_time_str[0] else 0\n",
    "    minutes = int(cook_time_str[1]) if cook_time_str[1] else 0\n",
    "    cook_time_minutes = hours * 60 + minutes\n",
    "\n",
    "    hours = int(prep_time_str[0]) if prep_time_str[0] else 0\n",
    "    minutes = int(prep_time_str[1]) if prep_time_str[1] else 0\n",
    "    prep_time_minutes = hours * 60 + minutes\n",
    "\n",
    "\n",
    "    hours = int(total_time_str[0]) if total_time_str[0] else 0\n",
    "    minutes = int(total_time_str[1]) if total_time_str[1] else 0\n",
    "    total_minutes = hours * 60 + minutes\n",
    "    #total = re.findall(r'Total:\\s*(\\d+)\\s*mins', lines[0])[0]\n",
    "    #servings = re.findall(r'Servings?:\\s*(\\d+?)\\s*(?:to\\s*\\d+)?\\s*(?:servings|ratings)', lines[0])[0] # sometimes instead of saying servings 6 they say servings 6 to 8 in this case we make it servings 6\n",
    "    #servings = re.findall(r'servings?:\\s*(\\d+?)\\s*(?:to\\s*\\d+)?\\s*(?:servings?|ratings)', lines[0], re.IGNORECASE)[0]\n",
    "    #text = \"The serving size is 3 servings per container.\"\n",
    "    \n",
    "    if(lines[0].count('serv')):\n",
    "        match = re.search(r'serv\\w*:\\D*(\\d+)', lines[0], re.IGNORECASE)\n",
    "        if match:\n",
    "            servings=(match.group(1))\n",
    "    else:\n",
    "        servings=1\n",
    "\n",
    "    \n",
    "\n",
    "    # Create a pandas DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Prep': [prep_time_minutes],\n",
    "        'Cook': [cook_time_minutes],\n",
    "        'Total': [total_minutes],\n",
    "        'Servings': [servings]\n",
    "    })\n",
    "    df = df.astype(int)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the second block of information, we will now focus on extracting the dish rating. This part posed its own challenges since the rating is displayed in the form of stars, with increments of 0.5. Instead of a numerical value, we need to determine the count of full stars and half stars to accurately represent the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stars(soup_obj):    \n",
    "    soup = soup_obj\n",
    "    results_items = soup.find_all(class_='comp js-feedback-trigger aggregate-star-rating mntl-block')    \n",
    "    #print(results_items.prettify())\n",
    "    for item in results_items:##result items size is 1\n",
    "        text=item.prettify()\n",
    "        full_stars=text.count('class=\"active\"')\n",
    "        half_stars=text.count('class=\"half\"')\n",
    "        return(full_stars+0.5*half_stars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up next is extracting the rating count, which doesn't pose any significant challenges. We can straightforwardly gather the number of ratings for each recipe without encountering any notable complexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_count(soup_obj):\n",
    "    soup = soup_obj\n",
    "    rating_elements = soup.find_all(\"div\", attrs={'class': \"comp aggregate-star-rating__count mntl-aggregate-rating mntl-text-block\"})\n",
    "    for rating_element in rating_elements:\n",
    "        rating_text = rating_element.text.strip()\n",
    "        try:\n",
    "            num_ratings = int(rating_text.split()[0])\n",
    "            return num_ratings\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the third block of information, we will now focus on extracting the nutritional values. However, we encountered a particular issue in this step. Alcoholic beverages listed on the site do not provide nutritional values, and unfortunately, there were numerous such recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nutritional_values(soup_obj):\n",
    "    soup = soup_obj\n",
    "    results_items = soup.find_all(class_='nutrition-info__table--row')\n",
    "\n",
    "    nutritional_vals=[]    \n",
    "\n",
    "\n",
    "    for item in results_items:\n",
    "        nutritional_vals.append(item.text.strip())\n",
    "    new_list = []\n",
    "    for s in nutritional_vals:\n",
    "        # Split the string by the \\n character and add the two parts to a new list\n",
    "        parts = s.split('\\n')\n",
    "        # Add the new strings to the new list in the desired format\n",
    "        #[caleories:934,fat:134g,carbs:999]\n",
    "        new_list.extend([parts[1], parts[0]])\n",
    "    #[calories,934,far,134g,carbs,1123,]\n",
    "    df = pd.DataFrame({'nutrient': new_list[::2], 'value': new_list[1::2]})\n",
    "\n",
    "    # Set 'nutrient' column as index and transpose DataFrame\n",
    "    df = df.set_index('nutrient').T\n",
    "    if df.empty: #recepies like cocktails have no calories\n",
    "\n",
    "        df = pd.DataFrame(columns=['nutrient', 'Calories', 'Fat', 'Carbs', 'Protein'])\n",
    "\n",
    "        # add a row filled with zeros\n",
    "        df.loc[0] = ['value', 0, '0g', '0g', '0g']\n",
    "        df['Calories'] = df['Calories'].astype(np.int32)\n",
    "        return df \n",
    "\n",
    "    df['Calories'] = df['Calories'].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the fourth block of relevant information, we will now focus on extracting the list of ingredients. Unfortunately, this step proved to be quite challenging. The class names for some recipes were inconsistent and changed over time, leading to numerous hotfixes and extensive debugging. To address this issue, we introduced a condition to check whether it is the old variant of the site or the new one. In some cases, we were able to filter the ingredients from the beginning if it was the new variant, simplifying the process to some extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingridients(soup_obj):\n",
    "    cond=0\n",
    "    soup = soup_obj\n",
    "    span_elements = soup.find_all('span', {'data-ingredient-name': 'true'})\n",
    "\n",
    "    # create an empty list to store the ingredient names\n",
    "    ingredient_names = []\n",
    "\n",
    "    # loop over the span elements and extract their text content\n",
    "    for span in span_elements:\n",
    "        ingredient_names.append(span.text)\n",
    "    #print(ingredient_names)\n",
    "    if(len(ingredient_names)>0):\n",
    "        return(ingredient_names)\n",
    "    else:\n",
    "        cond=0\n",
    "        soup = soup_obj\n",
    "        results_items = soup.find_all(class_='structured-ingredients__list text-passage')\n",
    "                                            #comp ingredient-list simple-list simple-list--bulleted  \n",
    "        #print(results_items)                                           \n",
    "        if(results_items==[]): #sometimes they like to change the class name\n",
    "            soup = soup_obj\n",
    "            results_items = soup.find_all(class_='simple-list__item js-checkbox-trigger ingredient text-passage')\n",
    "            cond=1\n",
    "        nutritional_vals=[]\n",
    "        if(results_items==[]):\n",
    "            return []\n",
    "        \n",
    "        final_lst=[]\n",
    "\n",
    "        for item in results_items:    \n",
    "            nutritional_vals.append(item.text.strip())\n",
    "            #print(item.text.strip())\n",
    "        if(cond==1):\n",
    "            return nutritional_vals\n",
    "        else:        \n",
    "            for i in nutritional_vals:\n",
    "                my_list = [s.strip() for s in i.split('\\n\\n\\n')]\n",
    "                final_lst.extend(my_list)\n",
    "                #print(final_lst)\n",
    "            \n",
    "            return(final_lst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we wanted to incorporate some additional information based on the ingredients. We examined the ingredients and attempted to determine if they were classified as dairy, meat, fur (parve), or a combination of dairy and meat. To identify common meat and dairy products, we conducted online searches and included the first few results as keywords for classification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_recipe(ingredients):\n",
    "    dairy_keywords = [\"milk\", \"cheese\", \"yogurt\", \"cream\", \"butter\", \"whey\", \"casein\", \"curds\"]\n",
    "    meat_keywords = [\"beef\", \"chicken\", \"pork\", \"lamb\", \"turkey\", \"venison\", \"duck\", \"bacon\", \"sausage\",\n",
    "                     \"ham\", \"prosciutto\", \"pepperoni\", \"salami\", \"chorizo\", \"bresaola\", \"pastrami\",\n",
    "                     \"corned beef\", \"veal\", \"goose\", \"game\", \"elk\", \"bison\", \"rabbit\", \"boar\", \"guinea fowl\", \"quail\"]\n",
    "    \n",
    "    categories = {'Dairy': 0, 'Meat': 0, 'Fur': 1}\n",
    "    \n",
    "    for ingredient in ingredients:\n",
    "        ingredient = ingredient.lower()\n",
    "        if any(keyword in ingredient for keyword in dairy_keywords):\n",
    "            categories['Dairy'] = 1\n",
    "            categories['Fur'] = 0\n",
    "        elif any(keyword in ingredient for keyword in meat_keywords):\n",
    "            categories['Meat'] = 1\n",
    "            categories['Fur'] = 0\n",
    "            \n",
    "    df = pd.DataFrame(categories, index=[0])\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to merge all the gathered data into a single row of a dataframe. This step will involve looping over the extracted information for each recipe and consolidating it into a unified format. By doing so, we will have a comprehensive dataframe that encapsulates all the relevant details for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fast(url,recepie_name):\n",
    "    df = pd.DataFrame(columns=['Name','Prep', 'Cook', 'Total', 'Servings', 'Rating','Rating_Count','Dairy','Meat','Fur', 'Calories', 'Fat', 'Carbs', 'Protein', 'Ingredients'])\n",
    "    #Dairy\n",
    "    #Meat\n",
    "    #Fur\n",
    "    soup_obj= load_soup_object(url)\n",
    "\n",
    "    recipe_df = get_cook_times(soup_obj)\n",
    "\n",
    "    ratings_list = get_stars(soup_obj)\n",
    "\n",
    "    nutrition_df = get_nutritional_values(soup_obj)\n",
    "\n",
    "    ingredients = get_ingridients(soup_obj)\n",
    "\n",
    "    rating_num = get_rating_count(soup_obj)\n",
    "\n",
    "    meatdairy_df = analyze_recipe(ingredients)\n",
    "\n",
    "    if(ingredients==[]):\n",
    "        ingredients=['','','','']\n",
    "        print(type(nutrition_df['Calories'][0]))\n",
    "    new_row = {\n",
    "        'Name':recepie_name,\n",
    "        'Prep': recipe_df['Prep'][0],\n",
    "        'Cook': recipe_df['Cook'][0],\n",
    "        'Total': recipe_df['Total'][0],\n",
    "        'Servings': recipe_df['Servings'][0],\n",
    "        'Rating': ratings_list,\n",
    "        'Rating_Count':rating_num,\n",
    "        'Dairy':meatdairy_df['Dairy'][0],\n",
    "        'Meat':meatdairy_df['Meat'][0],\n",
    "        'Fur':meatdairy_df['Fur'][0],\n",
    "        'Calories': nutrition_df['Calories'][0],\n",
    "        'Fat': nutrition_df['Fat'][0],\n",
    "        'Carbs': nutrition_df['Carbs'][0],        \n",
    "        'Protein': nutrition_df['Protein'][0],\n",
    "        'Ingredients': [ingredients]\n",
    "    }\n",
    "    #print(new_row)\n",
    "\n",
    "    # add the new row to the DataFrame\n",
    "    df = pd.concat([df, pd.DataFrame(new_row)], ignore_index=True)\n",
    "    #df_concat = pd.concat([df1, df2], keys=['df2'])\n",
    "    df['Prep'] = df['Prep'].astype(float)\n",
    "    df['Cook'] = df['Cook'].astype(float)\n",
    "    df['Total'] = df['Total'].astype(float)\n",
    "    df['Servings'] = df['Servings'].astype(float)\n",
    "    df['Rating'] = df['Rating'    ].astype(float)\n",
    "    df['Calories'] = df['Calories'].astype(float)\n",
    "\n",
    "    # display the updated DataFrame\n",
    "    #print(\"another line added\")\n",
    "    return(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement another function that performs the above-mentioned process for each recipe found thus far. As this operation can take some time, we aim to optimize future work by saving the results to a file. This way, we can have the data readily accessible for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_scrape(csv_file_name):\n",
    "    recipe_names = []\n",
    "    recipe_links = []\n",
    "    count=0\n",
    "\n",
    "    with open(csv_file_name, encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            recipe = ','.join(row).strip().replace('\\x9c', '')  # Join recipe name and link with a comma, remove problematic characters\n",
    "            last_comma = recipe.rfind(',')  # Find the index of the last comma in the recipe string\n",
    "            if last_comma != -1:\n",
    "                recipe_name = recipe[:last_comma].strip()  # Get the recipe name before the last comma\n",
    "                recipe_link = recipe[last_comma+1:].strip()  # Get the recipe link after the last comma\n",
    "                recipe_names.append(recipe_name)\n",
    "                recipe_links.append(recipe_link)\n",
    "            else:\n",
    "                print(f\"Invalid row: {row}\")\n",
    "\n",
    "    final_df=pd.DataFrame()\n",
    "    for name, link in zip(recipe_names, recipe_links):\n",
    "        # print(f\"Recipe name: {name}\")\n",
    "        # print(f\"Recipe link: {link}\")\n",
    "        temp_df=merge_fast(link,name)\n",
    "        temp_df.set_index('Name', inplace=True)  # set the index of temp_df to the recipe name\n",
    "        if(final_df.empty):\n",
    "            final_df=temp_df\n",
    "        else:\n",
    "            final_df = pd.concat([final_df,temp_df])\n",
    "        print(final_df)\n",
    "\n",
    "    #final_df.to_pickle('dataframe.pkl')\n",
    "    #final_df.to_pickle('dataframe.pkl', protocol=4, encoding='utf-8')\n",
    "    final_df.to_csv('my_data.csv', index=True, encoding='utf-8')\n",
    "    print(final_df)\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL 'Recipe_link': No scheme supplied. Perhaps you meant http://Recipe_link?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m fast_scrape(\u001b[39m'\u001b[39;49m\u001b[39mRecipe_Links_and_Names.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[12], line 23\u001b[0m, in \u001b[0;36mfast_scrape\u001b[1;34m(csv_file_name)\u001b[0m\n\u001b[0;32m     19\u001b[0m final_df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m name, link \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(recipe_names, recipe_links):\n\u001b[0;32m     21\u001b[0m     \u001b[39m# print(f\"Recipe name: {name}\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[39m# print(f\"Recipe link: {link}\")\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     temp_df\u001b[39m=\u001b[39mmerge_fast(link,name)\n\u001b[0;32m     24\u001b[0m     temp_df\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# set the index of temp_df to the recipe name\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m(final_df\u001b[39m.\u001b[39mempty):\n",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m, in \u001b[0;36mmerge_fast\u001b[1;34m(url, recepie_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mPrep\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCook\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTotal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mServings\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRating\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mRating_Count\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mDairy\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mMeat\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mFur\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCalories\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCarbs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mProtein\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIngredients\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[39m#Dairy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#Meat\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#Fur\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m soup_obj\u001b[39m=\u001b[39m load_soup_object(url)\n\u001b[0;32m      8\u001b[0m recipe_df \u001b[39m=\u001b[39m get_cook_times(soup_obj)\n\u001b[0;32m     10\u001b[0m ratings_list \u001b[39m=\u001b[39m get_stars(soup_obj)\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mload_soup_object\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_soup_object\u001b[39m(url):\n\u001b[0;32m      2\u001b[0m     \u001b[39m###\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[39m#url = \"https://www.thespruceeats.com/search?q=&searchType=recipe\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[0;32m      5\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m soup\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:573\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39m# Create the Request.\u001b[39;00m\n\u001b[0;32m    561\u001b[0m req \u001b[39m=\u001b[39m Request(\n\u001b[0;32m    562\u001b[0m     method\u001b[39m=\u001b[39mmethod\u001b[39m.\u001b[39mupper(),\n\u001b[0;32m    563\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    571\u001b[0m     hooks\u001b[39m=\u001b[39mhooks,\n\u001b[0;32m    572\u001b[0m )\n\u001b[1;32m--> 573\u001b[0m prep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_request(req)\n\u001b[0;32m    575\u001b[0m proxies \u001b[39m=\u001b[39m proxies \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    577\u001b[0m settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    578\u001b[0m     prep\u001b[39m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    579\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m     auth \u001b[39m=\u001b[39m get_netrc_auth(request\u001b[39m.\u001b[39murl)\n\u001b[0;32m    483\u001b[0m p \u001b[39m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 484\u001b[0m p\u001b[39m.\u001b[39;49mprepare(\n\u001b[0;32m    485\u001b[0m     method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(),\n\u001b[0;32m    486\u001b[0m     url\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl,\n\u001b[0;32m    487\u001b[0m     files\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mfiles,\n\u001b[0;32m    488\u001b[0m     data\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m    489\u001b[0m     json\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mjson,\n\u001b[0;32m    490\u001b[0m     headers\u001b[39m=\u001b[39;49mmerge_setting(\n\u001b[0;32m    491\u001b[0m         request\u001b[39m.\u001b[39;49mheaders, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders, dict_class\u001b[39m=\u001b[39;49mCaseInsensitiveDict\n\u001b[0;32m    492\u001b[0m     ),\n\u001b[0;32m    493\u001b[0m     params\u001b[39m=\u001b[39;49mmerge_setting(request\u001b[39m.\u001b[39;49mparams, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams),\n\u001b[0;32m    494\u001b[0m     auth\u001b[39m=\u001b[39;49mmerge_setting(auth, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth),\n\u001b[0;32m    495\u001b[0m     cookies\u001b[39m=\u001b[39;49mmerged_cookies,\n\u001b[0;32m    496\u001b[0m     hooks\u001b[39m=\u001b[39;49mmerge_hooks(request\u001b[39m.\u001b[39;49mhooks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhooks),\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    498\u001b[0m \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_method(method)\n\u001b[1;32m--> 368\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_url(url, params)\n\u001b[0;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:439\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39m*\u001b[39me\u001b[39m.\u001b[39margs)\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m scheme:\n\u001b[1;32m--> 439\u001b[0m     \u001b[39mraise\u001b[39;00m MissingSchema(\n\u001b[0;32m    440\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No scheme supplied. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerhaps you meant http://\u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m host:\n\u001b[0;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No host supplied\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL 'Recipe_link': No scheme supplied. Perhaps you meant http://Recipe_link?"
     ]
    }
   ],
   "source": [
    "data = fast_scrape('Recipe_Links_and_Names.csv')\n",
    "data.head()\n",
    "print(\"Describe the DataFrame:\\n\")\n",
    "data.describe()\n",
    "df = data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Data Analysis ####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataframe, let's explore and visualize the data to gain insights. To facilitate this process, we will create functions for different types of plots. Specifically, we will implement functions for scatter plots, histograms, and single-column pie charts. These functions will allow us to effectively visualize and analyze the data. Later, we can focus on cleaning and refining the visualizations for a more polished presentation :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter_2_params(df, col_name_1,col_name_2):\n",
    "    df['Fat'] = df['Fat'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Carbs'] = df['Carbs'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Protein'] = df['Protein'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    \n",
    "    df.plot.scatter(x=col_name_1, y=col_name_2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histo_1_params(df, col_name):\n",
    "    # read in your dataframe from a csv file\n",
    "   \n",
    "    df['Fat'] = df['Fat'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Carbs'] = df['Carbs'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Protein'] = df['Protein'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    # choose the column you want to use for the histogram\n",
    "\n",
    "    # sort the column values into bins\n",
    "    bin_values, bin_edges = np.histogram(df[col_name], bins='auto')\n",
    "\n",
    "    # create the histogram using the sorted bins\n",
    "    plt.hist(df[col_name], bins=bin_edges)\n",
    "\n",
    "    # add labels and title to the histogram\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of ' + col_name)\n",
    "\n",
    "    # display the histogram\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pie_1_params(col_name):\n",
    "    df = pd.read_csv('my_data.csv')\n",
    "    df['Fat'] = df['Fat'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Carbs'] = df['Carbs'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Protein'] = df['Protein'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    # choose the column you want to use for the pie chart    \n",
    "\n",
    "    # get the count of unique values in the column\n",
    "    value_counts = df[col_name].value_counts()\n",
    "\n",
    "    # create the pie chart\n",
    "    plt.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')\n",
    "\n",
    "    # add title to the pie chart\n",
    "    plt.title('Pie Chart of ' + col_name)\n",
    "\n",
    "    # display the pie chart\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the previously mentioned visualizations, we also want to include a pie chart illustrating the distribution of ingredients. Specifically, we aim to showcase the proportion of recipes that contain meat, dairy, a combination of meat and dairy, and fur (parve) ingredients. This pie chart will provide a clear overview of the composition of ingredients used in the recipes analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pie_meat_dairy_fur():\n",
    "    df = pd.read_csv('my_data.csv')\n",
    "    \n",
    "    # Calculate the number of recipes in each category\n",
    "    meat_count = len(df[df['Meat'] == 1])\n",
    "    dairy_count = len(df[df['Dairy'] == 1])\n",
    "    fur_count = len(df[df['Fur'] == 1])\n",
    "    dairy_meat_count = len(df[(df['Dairy'] == 1) & (df['Meat'] == 1)])\n",
    "\n",
    "    # Create a list of category counts and labels\n",
    "    counts = [meat_count, dairy_count, fur_count, dairy_meat_count]\n",
    "    labels = ['Meat', 'Dairy', 'Fur', 'Dairy&Meat']\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.pie(counts, labels=labels, autopct='%1.1f%%')\n",
    "\n",
    "    # Add a title to the chart\n",
    "    plt.title('Recipe Categories')\n",
    "\n",
    "    # Show the chart\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can now proceed to execute the functions for the different types of plots to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_all_histo(df):\n",
    "    numeric_cols = df.select_dtypes(include=['int', 'float']).columns.to_list()\n",
    "    # print(df.describe())\n",
    "    for col in numeric_cols:\n",
    "        draw_histo_1_params(df,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_all_histo(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the histogram of calories, we can observe that the most common calorie range for recipes is around 180 calories, which is the average value among approximately 700 recipes. Additionally, a significant portion of the recipes falls within the range of 0 to 500 calories.\n",
    "\n",
    "\n",
    "Similarly, for the histogram of carbohydrates, we observe a similar trend. The average recipe tends to contain carbohydrates within the range of 0 to 50 grams.\n",
    "\n",
    "\n",
    "Analyzing the histogram of the number of ingredients, we observe a normal distribution pattern. The average recipe contains around 10 ingredients. The majority of recipes fall within the range of 3 to 25 ingredients, indicating that most recipes can be prepared with a moderate number of ingredients. However, there are a few outliers with more than 27 ingredients, suggesting that some recipes may be more complex and require a wider variety of components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_scatter_2_params(df, 'Cook','Total')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot comparing Total Time and Cook Time, we can observe a linear relationship between the two variables. As the Total Time increases, the Cook Time also tends to increase in a consistent manner. This linear correlation indicates that as the overall cooking duration lengthens, the time required for the actual cooking process also extends accordingly. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the presence of outliers in our data, we will employ a data cleaning approach using the IQR (Interquartile Range) method. By applying this method to the entire dataframe, we can systematically identify and handle outliers across multiple columns. Additionally, we can perform careful adjustments to certain columns, removing unnecessary letters or characters, to convert them into numeric columns (e.g., fat, carbs). This process will help us clean and prepare the data for further analysis, ensuring more accurate and reliable results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df,column_name):\n",
    "    \n",
    "    print(df[column_name].describe())\n",
    "    # Calculate the IQR of the Prep column\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Determine the upper and lower bounds for the Prep column\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 10 * IQR\n",
    "\n",
    "    # Replace any values outside of the bounds with NaN\n",
    "    df.loc[(df[column_name] < lower_bound) | (df[column_name] > upper_bound), column_name] = float('NaN')\n",
    "    #df = df.dropna(subset=[column_name])\n",
    "    df.dropna(inplace=True)\n",
    "    print(df[column_name].describe())\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data cleaning process, we will establish a lower boundary of 1.5 times the IQR (Interquartile Range) and an upper boundary of 10 times the IQR. The upper boundary is set relatively high to account for recipes with exceptionally long cooking times, such as those exceeding 8 hours. It is essential to include these legitimate recipes in our analysis. By applying these boundaries, we can identify and handle outliers effectively, ensuring that our cleaned dataset maintains a reasonable range of values while still accounting for the diversity of cooking times present in the recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_g_fron_col(df,col_name):\n",
    "    df[col_name] = df[col_name].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    df=remove_g_fron_col(df,'Fat')\n",
    "    df=remove_g_fron_col(df,'Carbs')\n",
    "    df=remove_g_fron_col(df,'Protein')\n",
    "\n",
    "    \n",
    "    #print(df)\n",
    "    numeric_cols = df.select_dtypes(include=['int', 'float']).columns\n",
    "    #print(numeric_cols)\n",
    "    # # loop over the selected columns\n",
    "    for col in numeric_cols:\n",
    "        clean(df,col)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df(df)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conducting a detailed examination of the data frame, we have identified that certain rows contain incorrect values for \"Prep,\" \"Cook,\" and \"Total\" time. To rectify this issue, we will implement a simple function to clean the time values for each row. The function will attempt to correct the time values to their appropriate formats whenever possible. However, if a row's time values cannot be corrected, it will be removed from the data frame to ensure data accuracy and consistency. This approach will help us maintain a reliable and clean dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_time(df):\n",
    "    #df = pd.read_csv('clean_df.csv')\n",
    "\n",
    "    # Replace Prep, Cook, and Total values with median if Prep + Cook != Total\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Prep'] + row['Cook'] != row['Total']:\n",
    "            if row['Prep'] != 0:\n",
    "                df.at[index, 'Cook'] = df.at[index, 'Total'] - df.at[index, 'Prep']\n",
    "            elif row['Cook'] != 0:\n",
    "                df.at[index, 'Prep'] = df.at[index, 'Total'] - df.at[index, 'Cook']\n",
    "            else:\n",
    "                df.drop(index, inplace=True)\n",
    "\n",
    "    # Fill missing Total values with Prep + Cook\n",
    "    df['Total'].fillna(df['Prep'] + df['Cook'], inplace=True)\n",
    "\n",
    "    # Fill missing Prep or Cook values if only one is missing\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['Prep']):\n",
    "            if pd.notna(row['Cook']) and pd.notna(row['Total']):\n",
    "                df.at[index, 'Prep'] = df.at[index, 'Total'] - df.at[index, 'Cook']\n",
    "            else:\n",
    "                df.drop(index, inplace=True)\n",
    "        elif pd.isna(row['Cook']):\n",
    "            if pd.notna(row['Prep']) and pd.notna(row['Total']):\n",
    "                df.at[index, 'Cook'] = df.at[index, 'Total'] - df.at[index, 'Prep']\n",
    "            else:\n",
    "                df.drop(index, inplace=True)\n",
    "        elif pd.isna(row['Total']):\n",
    "            if pd.notna(row['Prep']) and pd.notna(row['Cook']):\n",
    "                df.at[index, 'Total'] = df.at[index, 'Prep'] + df.at[index, 'Cook']\n",
    "            else:\n",
    "                df.drop(index, inplace=True)\n",
    "\n",
    "    df = df[(df['Prep'] >= 0) & (df['Cook'] >= 0) & (df['Total'] >= 0)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data_time(df)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a clean and corrected data frame, we can now re-evaluate the results and draw meaningful conclusions from the refined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_all_histo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pie_meat_dairy_fur(df):\n",
    "\n",
    "    #df = pd.read_csv('my_data.csv')\n",
    "    \n",
    "    # Calculate the number of recipes in each category\n",
    "    meat_count = len(df[df['Meat'] == 1])\n",
    "    dairy_count = len(df[df['Dairy'] == 1])\n",
    "    fur_count = len(df[df['Fur'] == 1])\n",
    "    dairy_meat_count = len(df[(df['Dairy'] == 1) & (df['Meat'] == 1)])\n",
    "\n",
    "    # Create a list of category counts and labels\n",
    "    counts = [meat_count, dairy_count, fur_count, dairy_meat_count]\n",
    "    labels = ['Meat', 'Dairy', 'Fur', 'Dairy&Meat']\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.pie(counts, labels=labels, autopct='%1.1f%%')\n",
    "\n",
    "    # Add a title to the chart\n",
    "    plt.title('Recipe Categories')\n",
    "\n",
    "    # Show the chart\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_pie_meat_dairy_fur(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the distribution of ingredients in the cleaned data frame, we can now observe the proportion of recipes falling into different categories: 'Meat', 'Dairy', 'Fur' (parve), and 'Dairy&Meat' (a combination of dairy and meat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_3d(df):\n",
    "\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    xdata = df['Rating']\n",
    "    ydata = df['Calories']\n",
    "    zdata = df['Total']\n",
    "\n",
    "    ax.set_xlabel('Rating')\n",
    "    ax.set_ylabel('Calories')\n",
    "    ax.set_zlabel('Total time')\n",
    "\n",
    "    ax.scatter3D(xdata, ydata, zdata, c=zdata, depthshade=False)\n",
    "    plt.show()\n",
    "\n",
    "scatter_3d(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the visualization of recipe ratings, we observe that the ratings range from 0 to 5 stars, with increments of 0.5. Additionally, we can examine the distribution of recipes based on their rating, total cooking time, and calorie content. These visualizations help us understand the distribution and relationships between these parameters in our data frame.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Correlations :) ####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, to determine the correlations between different parameters in our data frame, we will utilize the following approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(df, col1, col2):\n",
    "    return df[col1].corr(df[col2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run this with each and every column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import correlation\n",
    "\n",
    "\n",
    "def get_corr_all_columns(df):\n",
    "    df = pd.read_csv('clean_modified.csv')\n",
    "    numeric_cols = df.select_dtypes(include=['int', 'float'])\n",
    "    if 'Unnamed' in numeric_cols.columns:\n",
    "        numeric_cols = numeric_cols.drop('Unnamed', axis=1)\n",
    "\n",
    "    correlation = {}\n",
    "    for col1 in numeric_cols:\n",
    "        for col2 in numeric_cols:\n",
    "            if col1 != col2:\n",
    "                corr_value = get_correlation(df, col1, col2)\n",
    "                if corr_value > 0.5 or corr_value < -0.5:\n",
    "                    correlation[(col1, col2)] = corr_value\n",
    "\n",
    "    # Sort the correlations by their absolute value, in descending order\n",
    "    sorted_correlations = sorted(correlation.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "    # Print the correlations above 0.5 or below -0.5\n",
    "    for corr, value in sorted_correlations:\n",
    "        if abs(value) > 0.5:\n",
    "            print(f\"{corr[0]} and {corr[1]}: {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enhance the visualization and facilitate better analysis, we will create two heatmaps based on the correlation matrix:\n",
    "\n",
    "Heatmap with all data: This heatmap will display the correlations between all pairs of columns in the data frame. It will provide a comprehensive overview of the relationships among all variables.\n",
    "\n",
    "Heatmap with strong correlations: This heatmap will focus on displaying only the strong correlations between variables. By setting a threshold for correlation strength (e.g., 0.5 or higher), we can highlight the significant relationships and identify the most influential factors within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def draw_heatmap_view(df):\n",
    "    heat_map_view = df[['Prep', 'Cook', 'Total', 'Servings', 'Rating', 'Rating_Count', 'Dairy', 'Meat', 'Fur', 'Calories','Fat','Carbs','Protein']]\n",
    "    sns.heatmap(heat_map_view.corr(), annot=True)\n",
    "    plt.show()\n",
    "\n",
    "def draw_heatmap_view_important(df):\n",
    "    heat_map_view = df[['Prep', 'Cook', 'Total', 'Servings', 'Rating', 'Rating_Count', 'Dairy', 'Meat', 'Fur', 'Calories','Fat','Carbs','Protein']]\n",
    "    corr = heat_map_view.corr()\n",
    "    mask = (corr > 0.5) | (corr < -0.5)  # set the threshold for correlation values\n",
    "    sns.heatmap(corr, annot=True, mask=~mask, cmap='coolwarm')\n",
    "    \n",
    "    # print the correlations above 0.5 or below -0.5\n",
    "    print(\"Correlations above 0.5 or below -0.5:\")\n",
    "    for i in range(len(corr.columns)):\n",
    "        for j in range(i):\n",
    "            if mask.iloc[i, j]:\n",
    "                print(f\"{corr.index[i]} - {corr.columns[j]}: {corr.iloc[i, j]}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_heatmap_view(df)\n",
    "draw_heatmap_view_important(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It appears that none of the initial parameters we examined in our analysis have a significant correlation with the 'Rating' column. This suggests that the factors we initially considered may not have a direct impact on the recipe rating. While this outcome may be unexpected, lets continue and find out what might affect the Rating."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the factors that contribute to the popularity of a recipe and evaluate the predictability of a recipe's popularity based on these factors, we will begin by filtering the ingredients. Our assumption is that certain ingredients may have a positive or negative impact on the recipe's rating.\n",
    "\n",
    "With this in mind, we will initiate the cleaning process by applying filters to the ingredients. By categorizing the ingredients based on their potential influence on the recipe's popularity, we can isolate and analyze the impact of specific ingredients or ingredient groups. This approach will help us understand the relationship between ingredients and recipe popularity more effectively.\n",
    "\n",
    "Let's proceed with the cleaning process and investigate how different ingredients may affect the overall popularity of recipes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Ingredients ####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what we are working with by using a simple plot to see the most frequent ingredients in the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_data_review(df):\n",
    "    #Create a list of all the ingredients\n",
    "    #df=pd.read_csv('my_data.csv')\n",
    "    all_ingredients = []\n",
    "\n",
    "    for i in df['Ingredients']:\n",
    "        all_ingredients += eval(i)\n",
    "    # Define a list of terms to exclude from the ingredients list\n",
    "    exclude_terms = ['salt', 'pepper', 'garlic', 'onion', 'paprika', 'cumin', 'chili', 'oregano', 'basil', 'thyme', 'rosemary']\n",
    "    # Create a list of all the ingredients\n",
    "    clean_ingredients = []\n",
    "    for ingredient in all_ingredients:\n",
    "        ingredient = re.sub(r'\\d+(\\.\\d+)?', '', ingredient) # Remove any quantity\n",
    "        ingredient = re.sub(r'(\\s+\\d+)?\\s*(large|medium|small)?\\s*(cup|teaspoon|tablespoon)s?', '', ingredient, flags=re.IGNORECASE) # Remove any volume/capacity description\n",
    "        ingredient = ingredient.strip()\n",
    "        if ingredient and not any(term in ingredient.lower() for term in exclude_terms):\n",
    "            clean_ingredients.append(ingredient)\n",
    "\n",
    "    # Count the frequency of each ingredient\n",
    "    ingredient_counts = pd.Series(clean_ingredients).value_counts()\n",
    "\n",
    "    # Create a pie chart for the top 10 ingredients\n",
    "    top_10_ingredients = ingredient_counts.head(10)\n",
    "    plt.pie(top_10_ingredients, labels=top_10_ingredients.index, autopct='%1.1f%%')\n",
    "    plt.title('Top 10 Ingredients in Recipes (Excluding Spices)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data_review(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the initial plot of the most frequent ingredients did not provide much insight, it was important to clean the data as many ingredients had additional information such as volume or weight that we did not need for our analysis. Additionally, there were discrepancies in the ingredient names, such as 'Wheat Flour' and 'Corn Flour' that should both be considered as 'Flour'. This type of data cleaning is crucial to ensure the accuracy and reliability of our analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea was to make a dictionary of the top X ingridients and manully filter the junk out of them first lets make the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def find_most_common_ingridients(df):\n",
    "    final_lst = []\n",
    "    #df = pd.read_csv('clean.csv')\n",
    "    ingredients = df['Ingredients'].tolist()\n",
    "\n",
    "    for item in ingredients:\n",
    "        temp = item.replace(\"'\", \"\")   \n",
    "        temp = item.replace(\"\\\\\\\\xa0\", \" \")\n",
    "        temp = item.replace(\"\\\\\\\\u200b\", \" \")\n",
    "        temp = temp.strip(\"[]\").split(\", \")\n",
    "        temp_tokens = []\n",
    "        for item in temp:\n",
    "            temp_tokens.extend(item.split())\n",
    "        final_lst.extend(temp_tokens)\n",
    "\n",
    "    #print(final_lst)\n",
    "    word_count = collections.Counter(final_lst)\n",
    "\n",
    "    # Filter out items that start with a digit\n",
    "    sorted_dict = {k: v for k, v in word_count.items() if not k[0].isdigit()}\n",
    "    sorted_dict = dict(sorted(sorted_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    print('\\n\\n\\n\\n')\n",
    "    # Print the top 100 items\n",
    "\n",
    "    # Create a DataFrame from the sorted dictionary and save it as a CSV file\n",
    "    df = pd.DataFrame.from_dict(sorted_dict, orient='index', columns=['count'])\n",
    "    df.to_csv('Ingredients_dirty.csv')\n",
    "    \n",
    "    for i, (key, value) in enumerate(sorted_dict.items()):\n",
    "        if i == 300:\n",
    "            break\n",
    "        print(f\"{key}: {value}\")\n",
    "    return sorted_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To streamline the data cleaning process, we utilized the power of text editors like Notepad++ and applied regular expressions (regex) to manipulate the ingredient data. Here are the steps we followed:\n",
    "\n",
    "Lowercasing: We converted all the ingredient names to lowercase to ensure consistency and eliminate any case-related discrepancies.\n",
    "\n",
    "Removing cells with numbers: Since we wanted to focus on ingredient names rather than quantities or measurements, we removed any cells that contained numbers.\n",
    "\n",
    "Removing duplicates: We eliminated duplicate ingredient entries to avoid redundancy in our analysis.\n",
    "\n",
    "Clustering similar ingredients: We manually reviewed the ingredient list and grouped together ingredients that were essentially the same but had slight variations in their names. For example, 'Wheat Flour' and 'Corn Flour' were clustered as 'Flour' since they both belong to the same category.\n",
    "\n",
    "These manual steps were necessary due to the complexity and variations in ingredient names. By carefully curating and organizing the ingredient data, we can now proceed with a cleaner and more consistent dataset for further analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(figure out how to upload a csv with the notebook we need to upload the ingridients)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our set of relevant ingredients, it is time to filter the original ingredient data. Our goal is to replace any ingredient that appears as a substring in the original ingredients with the corresponding ingredient name from the new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ingridients():\n",
    "    with open('ingridients.csv', 'r') as file:\n",
    "    # Initialize an empty list to store the data\n",
    "        data = []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            values = line.split(',')\n",
    "            return values\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_index_columns(df):\n",
    "    # Get a list of all column names that start with 'Unnamed'\n",
    "    index_cols = [col for col in df.columns if col.startswith('Unnamed')]\n",
    "    df.drop(columns=index_cols, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the data cleaning process, we observed that the word 'tea' appeared frequently in the ingredient lists. This occurrence was a result of the word 'Teaspoon' being transformed to 'tea' after applying our clean_ingredients function. To address this issue and ensure accurate representation, we decided to further clean the ingredient lists by removing the term 'Teaspoon' from the ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_teaspoon(df, column):\n",
    "  df=delete_index_columns(df)\n",
    "  df[column] = df[column].str.replace('teaspoon', '')\n",
    "  df=delete_index_columns(df)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ingridients(df):\n",
    "    df=delete_index_columns(df)\n",
    "    df2=df.copy()\n",
    "    pure_ingredients=read_ingridients()\n",
    "    print(\"starting ingridient cleanup this might take a minute\")\n",
    "    #df=pd.read_csv(filename)\n",
    "    for i, ingredient in enumerate(df2['Ingredients']):\n",
    "        line = ingredient.split(\",\")\n",
    "        for j, item in enumerate(line):\n",
    "            for single_ingredient in pure_ingredients:\n",
    "                if single_ingredient.lower() in item.lower():\n",
    "                    #print(f'found {single_ingredient} in {item}')\n",
    "                    line[j] = single_ingredient.lower()\n",
    "        df2.loc[i, 'Ingredients'] = \",\".join(line)\n",
    "        #print(line)\n",
    "        #print('\\n\\n')\n",
    "    #print(df['Ingredients'])\n",
    "    # save the modified dataframe to a new CSV file\n",
    "    df2=delete_index_columns(df)\n",
    "    #df2.to_csv('clean_modified.csv', index=False)\n",
    "    print(\"done!\")\n",
    "   \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_teaspoon(df,'Ingredients')# Removing the teaspoon word from the list of ingredients\n",
    "df = clean_ingridients(df)# Cleaning the data by ingredients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets visualise this and hope that we cleaned it properly. We can see now what is the most frequent ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingridients_dict(df):\n",
    "    ingredients = []\n",
    "\n",
    "    with open('ingridients.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            ingredients.extend(row)\n",
    "\n",
    "    #df= pd.read_csv('clean_modified.csv')\n",
    "\n",
    "\n",
    "    ingredients_dict = {}\n",
    "\n",
    "    # loop through the ingredients in the DataFrame\n",
    "    for line in df['Ingredients']:\n",
    "        for single_item in line.split(','): # split the ingredients by comma if they're in a single string\n",
    "            for ing_in_lst in ingredients: # loop through the ingredients in the ingredients list\n",
    "                if ing_in_lst in single_item.strip(): # check if the ingredient in the list is a substring of the ingredient in the dataset\n",
    "                    # if the ingredient exists in the dictionary, increment its count\n",
    "                    if ing_in_lst in ingredients_dict:\n",
    "                        ingredients_dict[ing_in_lst] += 1\n",
    "                    # otherwise, add the ingredient to the dictionary with a count of 1\n",
    "                    else:\n",
    "                        ingredients_dict[ing_in_lst] = 1\n",
    "\n",
    "    # sort the dictionary by value in descending order\n",
    "    sorted_dict = dict(sorted(ingredients_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    #print the dictionary of ingredients and their counts\n",
    "    print(\"Printing the ingredient Counts:\")\n",
    "    for ing, count in sorted_dict.items():\n",
    "        print(f'{ing}: {count}')   \n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_ingridients_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Sorted_dict \u001b[39m=\u001b[39m get_ingridients_dict(df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_ingridients_dict' is not defined"
     ]
    }
   ],
   "source": [
    "Sorted_dict = get_ingridients_dict(df)\n",
    "print(\"Printing the ingredient Counts:\")\n",
    "for ing, count in Sorted_dict.items():\n",
    "  print(f'{ing}: {count}')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ingridient_pie_chart(top_n):\n",
    "    # print()\n",
    "    ingredient_counts = get_ingridients_dict()\n",
    "\n",
    "    # Extract the top N ingredients by frequency\n",
    "    ingredient_names = collections.Counter(ingredient_counts).most_common(top_n)\n",
    "    ingredient_names = [x[0] for x in ingredient_names]\n",
    "    ingredient_frequencies = [ingredient_counts[name] for name in ingredient_names]\n",
    "    total_receipe_count = sum(ingredient_frequencies)\n",
    "    ingredient_percentages = [(count/total_receipe_count)*100 for count in ingredient_frequencies]\n",
    "    ingredient_labels = ['{} ({:.1f}%)'.format(name, percentage) for name, percentage in zip(ingredient_names, ingredient_percentages)]\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.pie(ingredient_percentages, labels=ingredient_labels)\n",
    "    plt.title('Top {} Ingredients'.format(top_n))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like it works properly :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the potential correlation between ingredients and recipe ratings, we employed a technique called \"ingredient explosion.\" This technique involves expanding the ingredients into binary columns, where a value of 1 indicates the presence of an ingredient in a recipe, and 0 indicates its absence.\n",
    "\n",
    "By utilizing this approach, we can transform the ingredient data into a more structured format that allows us to analyze the relationship between individual ingredients and recipe ratings. The binary columns provide a way to quantify the presence or absence of specific ingredients for each recipe in the dataset.\n",
    "\n",
    "With the exploded ingredient columns in place, we can proceed with investigating the correlation between these ingredient indicators and the recipe ratings to gain insights into which ingredients may have an impact on the overall rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_ingridients_and_get_corr(df):#this function expands the dataframe to each ingridient and tries to find a correlation\n",
    "    #df = pd.read_csv('clean_modified.csv')\n",
    "\n",
    "    # extract the ingredients column\n",
    "    df_ing = df['Ingredients']\n",
    "\n",
    "    # define a list of ingredients you care about\n",
    "    important_ingredients = []\n",
    "\n",
    "    with open('ingridients.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            important_ingredients.extend(row)\n",
    "\n",
    "    # loop through the ingredients you care about\n",
    "    for ingredient in important_ingredients:\n",
    "        # create a new column with a value of 1 if the recipe contains the ingredient and 0 otherwise\n",
    "        df[ingredient] = df_ing.str.contains(ingredient).astype(int)\n",
    "\n",
    "    correlations = df[['Rating'] + [ingredient for ingredient in important_ingredients]].corr()\n",
    "\n",
    "    # print the correlation coefficients for each ingredient\n",
    "    # print(correlations.loc['Rating'])\n",
    "    top_20 = correlations.loc['Rating'].sort_values(ascending=False)[1:21]\n",
    "    print(\"Top 20 ingredients with the highest correlation with the 'Rating' column:\")\n",
    "    print(top_20)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= explode_ingridients_and_get_corr(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conducting an in-depth analysis, we discovered that there are no significant correlations between individual ingredients and recipe ratings. This finding provides an answer to one of our research questions, indicating that no specific ingredient can directly predict the rating of a recipe.\n",
    "\n",
    "This outcome emphasizes the complexity of determining what makes a recipe highly rated and underscores the importance of considering various factors such as preparation methods, cooking techniques, flavor combinations, and presentation when evaluating recipe quality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the natural next step is to try machine learning and see if it can predict the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating_linear(df):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Prepare X and y\n",
    "    X = df.drop(['Ingredients', 'Name', 'Rating'], axis=1)\n",
    "    y = df['Rating']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and fit the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Round predicted ratings to nearest half-integer value\n",
    "    def round_half_up(x):\n",
    "        # Clamp ratings to 0-5 range\n",
    "        x = np.clip(x, 0, 5)\n",
    "        # Round to nearest half-integer value\n",
    "        return np.floor(x * 2 + 0.5) / 2\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_pred_rounded = round_half_up(y_train_pred)\n",
    "\n",
    "    # Count correct and incorrect predictions in the training set\n",
    "    num_train_correct = np.sum(np.abs(y_train_pred_rounded - y_train) <= 0.25)\n",
    "    num_train_incorrect = len(y_train) - num_train_correct\n",
    "    percent_train_correct = num_train_correct / len(y_train) * 100\n",
    "\n",
    "    # Make predictions on the testing set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_rounded = round_half_up(y_test_pred)\n",
    "\n",
    "    # Count correct and incorrect predictions in the testing set\n",
    "    num_test_correct = np.sum(np.abs(y_test_pred_rounded - y_test) <= 0.25)\n",
    "    num_test_incorrect = len(y_test) - num_test_correct\n",
    "    percent_test_correct = num_test_correct / len(y_test) * 100\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Number of recipes in the dataset: {len(df)}\")\n",
    "    print(f\"Number of recipes in the training set: {len(X_train)}\")\n",
    "    print(f\"Number of recipes in the testing set: {len(X_test)}\")\n",
    "    print(f\"Number of correctly predicted recipes in the training set: {num_train_correct}\")\n",
    "    print(f\"Number of incorrectly predicted recipes in the training set: {num_train_incorrect}\")\n",
    "    print(f\"Percent of correctly predicted recipes in the training set: {percent_train_correct}%\")\n",
    "    print(f\"Number of correctly predicted recipes in the testing set: {num_test_correct}\")\n",
    "    print(f\"Number of incorrectly predicted recipes in the testing set: {num_test_incorrect}\")\n",
    "    print(f\"Percent of correctly predicted recipes in the testing set: {percent_test_correct}%\")\n",
    "\n",
    "    test_results = np.concatenate((y_test.to_numpy().reshape(-1, 1), y_test_pred_rounded.reshape(-1, 1)), axis=1)\n",
    "    plt.scatter(range(len(test_results)), test_results[:, 0], label='Actual Ratings')\n",
    "    plt.scatter(range(len(test_results)), test_results[:, 1], label='Predicted Ratings')\n",
    "    plt.xlabel('Recipe Number')\n",
    "    plt.ylabel('Rating')\n",
    "    plt.ylim(0, 5)  # Set y-axis limits\n",
    "    plt.title('Actual vs. Predicted Ratings')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return model, num_train_correct, num_train_incorrect, percent_train_correct, num_test_correct, num_test_incorrect, percent_test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rating_linear(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADD THE NUMBER IN ACCURACY####\n",
    "Those results werent looking that great. We will try to use a diifferent machine learning algorthims in order to improve our prediction. The next algorithms we will try to use is KNN. Lets quickly create a new fucntion and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating_knn(df):\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = df.drop(['Ingredients', 'Name', 'Rating'], axis=1)\n",
    "    y = df['Rating']\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create and fit the model\n",
    "    best_k = 0\n",
    "    best_accuracy = 0\n",
    "    for k in range(1, 31):\n",
    "        model = KNeighborsRegressor(n_neighbors=k)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "        # Make predictions on the training set\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_train_pred_rounded = np.round(np.clip(y_train_pred, 0, 5) * 2) / 2\n",
    "    \n",
    "        # Count correct and incorrect predictions in the training set\n",
    "        num_train_correct = np.sum(np.abs(y_train_pred_rounded - y_train) <= 0.25)\n",
    "        percent_train_correct = num_train_correct / len(y_train) * 100\n",
    "    \n",
    "        # Make predictions on the testing set\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_pred_rounded = np.round(np.clip(y_test_pred, 0, 5) * 2) / 2\n",
    "    \n",
    "        # Count correct and incorrect predictions in the testing set\n",
    "        num_test_correct = np.sum(np.abs(y_test_pred_rounded - y_test) <= 0.25)\n",
    "        percent_test_correct = num_test_correct / len(y_test) * 100\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"For k={k}:\")\n",
    "        print(f\"Number of correctly predicted recipes in the training set: {num_train_correct}\")\n",
    "        print(f\"Percent of correctly predicted recipes in the training set: {percent_train_correct}%\")\n",
    "        print(f\"Number of correctly predicted recipes in the testing set: {num_test_correct}\")\n",
    "        print(f\"Percent of correctly predicted recipes in the testing set: {percent_test_correct}%\")\n",
    "        \n",
    "        # Check if the current model is better than the previous best model\n",
    "        if percent_test_correct > best_accuracy:\n",
    "            best_k = k\n",
    "            best_accuracy = percent_test_correct\n",
    "    \n",
    "    print(f\"\\nBest k: {best_k}\")\n",
    "    print(f\"Best percent of correctly predicted recipes in the testing set: {best_accuracy}%\")\n",
    "    \n",
    "    # Train the best model\n",
    "    model = KNeighborsRegressor(n_neighbors=best_k)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the testing set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_rounded = np.round(np.clip(y_test_pred, 0, 5) * 2) / 2\n",
    "    \n",
    "    # Count correct and incorrect predictions in the testing set\n",
    "    num_test_correct = np.sum(np.abs(y_test_pred_rounded - y_test) <= 0.25)\n",
    "    num_test_incorrect = len(y_test) - num_test_correct\n",
    "    percent_test_correct = num_test_correct / len(y_test) * 100\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal results:\")\n",
    "    print(f\"Number of recipes in the dataset: {len(df)}\")\n",
    "    print(f\"Number of recipes in the training set: {len(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rating_knn(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADD THE NUMBER IN ACCURACY #####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This didnt provide with any better results. As lets resort we will try to implement a working Nural-Network to achieve a possible better results with implementing layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating_mlp(df):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from keras import layers\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    print(\"Preprocessing data...\")\n",
    "    # select features and target variable\n",
    "    X = df.drop(['Ingredients', 'Name', 'Rating'], axis=1)\n",
    "    y = df['Rating']\n",
    "\n",
    "    # scale data\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_scraped = scaler.transform(df.drop(['Ingredients', 'Rating', 'Name'], axis=1))\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # build model\n",
    "    print(\"Building model...\")\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'Root Mean Squared Error: {rmse:.3f}')\n",
    "\n",
    "    # Predict the ratings for the scraped data\n",
    "    y_pred = model.predict(X_scraped)\n",
    "\n",
    "    # Round the predicted ratings to the nearest 0.5\n",
    "    y_pred_rounded = (np.round(y_pred * 2) / 2)\n",
    "\n",
    "    # Print the predicted ratings and their accuracy compared to the actual ratings\n",
    "    print('Predicted Ratings:')\n",
    "    print(y_pred_rounded.flatten())\n",
    "    accuracy = r2_score(y_test, model.predict(X_test))\n",
    "    print(f'Accuracy compared to test data: {accuracy:.3f}')\n",
    "    \n",
    "    return y_pred_rounded.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rating_mlp(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our accuracy hovers around 0.36(to confirm) which isnt that good so we played around with feature engineering to try and improve our predictions. First we will try to scrape new data from our sites that being the amount of steps needed to complete a recipe named 'Steps' and the number of ingredients needed for the recipe named 'Num_ing'. \n",
    "\n",
    "We decide to also make a popularity column that would sum the frequncy of each ingridient in the recepie to try to get a better scroing for a recipe, not only that but we decided to make a difficulty column that takes into account both the steps, cooking time and number of ingredients to better detrmine how each recipe differes from one another.\n",
    "\n",
    "This means we will need to change our scraping functions and to run the cleannnig function on our new data as well :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steps_to_cook(soup_obj):\n",
    "    class_name = \"comp mntl-sc-block-group--LI mntl-sc-block mntl-sc-block-startgroup\"\n",
    "    elements = soup_obj.find_all(class_=class_name)\n",
    "\n",
    "    # Print the number of elements found\n",
    "    #print(f\"Number of instances of {class_name}: {len(elements)}\")\n",
    "    return len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fast(url,recepie_name):\n",
    "    df = pd.DataFrame(columns=['Name','Prep', 'Cook', 'Total', 'Servings', 'Rating','Rating_Count','Dairy','Meat','Fur', 'Calories', 'Fat', 'Carbs', 'Protein','Num_ing','Steps', 'Ingredients'])\n",
    "    #Dairy\n",
    "    #Meat\n",
    "    #Fur\n",
    "    soup_obj = load_soup_object(url)\n",
    "\n",
    "    recipe_df = get_cook_times(soup_obj)\n",
    "\n",
    "    ratings_list= get_stars(soup_obj)\n",
    "\n",
    "    nutrition_df = get_nutritional_values(soup_obj)\n",
    "\n",
    "    ingredients = get_ingridients(soup_obj)\n",
    "\n",
    "    rating_num = get_rating_count(soup_obj)\n",
    "\n",
    "    meatdaity_df = analyze_recipe(ingredients)\n",
    "\n",
    "    num_ing = len(ingredients)\n",
    "\n",
    "    steps = get_steps_to_cook(soup_obj)\n",
    "\n",
    "    if(ingredients==[]):\n",
    "        ingredients=['','','','']\n",
    "        print(type(nutrition_df['Calories'][0]))\n",
    "    new_row = {\n",
    "        'Name':recepie_name,\n",
    "        'Prep': recipe_df['Prep'][0],\n",
    "        'Cook': recipe_df['Cook'][0],\n",
    "        'Total': recipe_df['Total'][0],\n",
    "        'Servings': recipe_df['Servings'][0],\n",
    "        'Rating': ratings_list,\n",
    "        'Rating_Count':rating_num,\n",
    "        'Dairy':meatdaity_df['Dairy'][0],\n",
    "        'Meat':meatdaity_df['Meat'][0],\n",
    "        'Fur':meatdaity_df['Fur'][0],\n",
    "        'Calories': nutrition_df['Calories'][0],\n",
    "        'Fat': nutrition_df['Fat'][0],\n",
    "        'Carbs': nutrition_df['Carbs'][0],        \n",
    "        'Protein': nutrition_df['Protein'][0],\n",
    "        'Num_ing' : num_ing,\n",
    "        'Steps' : steps,      \n",
    "        'Ingredients': [ingredients]\n",
    "    }\n",
    "    #print(new_row)\n",
    "\n",
    "    # add the new row to the DataFrame\n",
    "    df = pd.concat([df, pd.DataFrame(new_row)], ignore_index=True)\n",
    "    #df_concat = pd.concat([df1, df2], keys=['df2'])\n",
    "    df['Prep'] = df['Prep'].astype(float)\n",
    "    df['Cook'] = df['Cook'].astype(float)\n",
    "    df['Total'] = df['Total'].astype(float)\n",
    "    df['Servings'] = df['Servings'].astype(float)\n",
    "    df['Rating'] = df['Rating'    ].astype(float)\n",
    "    df['Calories'] = df['Calories'].astype(float)\n",
    "\n",
    "    # display the updated DataFrame\n",
    "    #print(\"another line added\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_scrape(csv_file_name):\n",
    "    recipe_names = []\n",
    "    recipe_links = []\n",
    "    count=0\n",
    "\n",
    "    with open(csv_file_name, encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            recipe = ','.join(row).strip().replace('\\x9c', '')  # Join recipe name and link with a comma, remove problematic characters\n",
    "            last_comma = recipe.rfind(',')  # Find the index of the last comma in the recipe string\n",
    "            if last_comma != -1:\n",
    "                recipe_name = recipe[:last_comma].strip()  # Get the recipe name before the last comma\n",
    "                recipe_link = recipe[last_comma+1:].strip()  # Get the recipe link after the last comma\n",
    "                recipe_names.append(recipe_name)\n",
    "                recipe_links.append(recipe_link)\n",
    "            else:\n",
    "                print(f\"Invalid row: {row}\")\n",
    "\n",
    "    final_df=pd.DataFrame()\n",
    "    for name, link in zip(recipe_names, recipe_links):\n",
    "        # print(f\"Recipe name: {name}\")\n",
    "        # print(f\"Recipe link: {link}\")\n",
    "        temp_df=merge_fast(link,name)\n",
    "        temp_df.set_index('Name', inplace=True)  # set the index of temp_df to the recipe name\n",
    "        if(final_df.empty):\n",
    "            final_df=temp_df\n",
    "        else:\n",
    "            final_df = pd.concat([final_df,temp_df])\n",
    "        print(final_df)\n",
    "\n",
    "    #final_df.to_pickle('dataframe.pkl')\n",
    "    #final_df.to_pickle('dataframe.pkl', protocol=4, encoding='utf-8')\n",
    "    final_df.to_csv('my_data.csv', index=True, encoding='utf-8')\n",
    "    print(final_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to run the scraping all over again, as well as to clean the initial data as we did. Its a long process but nothing can be done about it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Updated_Data = fast_scrape('Recipe_Links_and_Names.csv')\n",
    "Updated_Data = clean_data_time(Updated_Data)\n",
    "Updated_Data = clean_df(Updated_Data)\n",
    "Updated_Data = remove_teaspoon(Updated_Data,'Ingredeints')\n",
    "Updated_Data = clean_ingridients(Updated_Data)\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_popularity_score_to_df(df):\n",
    "    binary_columns = [col for col in df.columns if col not in ['Name', 'Prep', 'Cook', 'Total', 'Servings', 'Rating', 'Rating_Count', 'Dairy', 'Meat', 'Fur', 'Calories', 'Fat', 'Carbs', 'Protein', 'Num_ing', 'Steps', 'Ingredients']]\n",
    "\n",
    "    # Calculate the frequencies of each binary value\n",
    "    ingredient_frequencies = df[binary_columns].sum() / len(df) * 30\n",
    "\n",
    "    # Calculate the popularity score for each recipe\n",
    "    popularity_scores = []\n",
    "    for _, row in df.iterrows():\n",
    "        score = 0\n",
    "        for column in binary_columns:\n",
    "            if row[column] == 1:\n",
    "                score += ingredient_frequencies[column]\n",
    "        popularity_scores.append(score)\n",
    "\n",
    "    # Add the popularity scores as a new column to the DataFrame\n",
    "    df['Popularity Score'] = popularity_scores\n",
    "\n",
    "    # Print the updated DataFrame\n",
    "    #print(df)\n",
    "    #scraping_functions.draw_histo_1_params(df,'Popularity Score')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_difficulty_column(df):\n",
    "    # Read the CSV file into a pandas dataframe\n",
    "    #df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Calculate the combined score for each recipe\n",
    "    df['Combined'] = df['Num_ing'] + df['Steps'] + df['Total']\n",
    "    \n",
    "    # Calculate the mean and standard deviation of the combined score\n",
    "    mean = df['Combined'].mean()\n",
    "    std_dev = df['Combined'].std()\n",
    "    \n",
    "    # Calculate the z-score for each recipe\n",
    "    df['Z_score'] = (df['Combined'] - mean) / std_dev\n",
    "    \n",
    "    # Divide the z-scores into 5 equal-sized groups\n",
    "    df['Difficulty'] = pd.qcut(df['Z_score'], q=5, labels=[1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Write the updated dataframe to a new CSV file\n",
    "    # new_csv_file_path = csv_file_path.split('.csv')[0] + '_with_difficulty.csv'\n",
    "    # df.to_csv(new_csv_file_path, index=False)\n",
    "    #df.to_csv(csv_file_path, index=False, mode='w')\n",
    "    #save_df(df,\"csv_Wtih_diff.csv\")\n",
    "    \n",
    "    # Print the number of recipes in each difficulty level\n",
    "    for i in range(1, 6):\n",
    "        count = df['Difficulty'][df['Difficulty'] == i].count()\n",
    "        print(f\"Difficulty level {i}: {count}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Updated_Data = add_difficulty_column(Updated_Data)\n",
    "Updated_Data = explode_ingridients_and_get_corr(Updated_Data)\n",
    "Updated_Data = add_popularity_score_to_df(Updated_Data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save the newly accuried data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df,name):\n",
    "    df.to_csv(name, index=True, encoding='utf-8')\n",
    "\n",
    "save_df(Updated_Data, 'FinalCSVFile.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if anything new appearded in regards to the correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_heatmap_view(df):\n",
    "    heat_map_view = df[['Prep', 'Cook', 'Total', 'Servings', 'Rating', 'Rating_Count', 'Dairy', 'Meat', 'Fur', 'Calories','Fat','Carbs','Protein','Num_ing','Steps','Z_score','Difficulty']]\n",
    "    sns.heatmap(heat_map_view.corr(), annot=True)\n",
    "    plt.show()\n",
    "\n",
    "def draw_heatmap_view_important(df):\n",
    "    heat_map_view = df[['Prep', 'Cook', 'Total', 'Servings', 'Rating', 'Rating_Count', 'Dairy', 'Meat', 'Fur', 'Calories','Fat','Carbs','Protein','Num_ing','Steps','Z_score','Difficulty']]\n",
    "    corr = heat_map_view.corr()\n",
    "    mask = (corr > 0.5) | (corr < -0.5)  # set the threshold for correlation values\n",
    "    sns.heatmap(corr, annot=True, mask=~mask, cmap='coolwarm')\n",
    "    \n",
    "    # print the correlations above 0.5 or below -0.5\n",
    "    print(\"Correlations above 0.5 or below -0.5:\")\n",
    "    for i in range(len(corr.columns)):\n",
    "        for j in range(i):\n",
    "            if mask.iloc[i, j]:\n",
    "                print(f\"{corr.index[i]} - {corr.columns[j]}: {corr.iloc[i, j]}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "draw_heatmap_view_important(Updated_Data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see nothing outstanding really appeared from the extra scraping and future enginering but lets see if our algorithm can be improved with this new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rating_linear(Updated_Data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, our latest machine learning model achieved a score of 0.47, indicating some improvement compared to our previous attempt. Although the score might not be considered high, it does demonstrate a notable 30% enhancement in predictive performance.\n",
    "\n",
    "During our exploration, we experimented with various feature engineering techniques to extract more meaningful information from the data. We carefully evaluated the impact of each feature and excluded columns and feature engineering attempts that did not yield significant results. This approach allowed us to focus on the most influential factors that could contribute to predicting recipe ratings accurately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing our Approach ####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After realizing our previous attempts to predict star ratings were not yielding satisfactory results, we decided to take a different approach. Instead of directly predicting star ratings, we focused on a simpler question: Can we determine if a recipe is \"good\" or not? We defined \"good\" as recipes with a 5-star rating. To answer this question, we used a Perceptron model, a binary classification model that determines if a recipe is 5 stars or not. By training the model on factors like cooking time, ingredients, and nutritional values, we aimed to identify what makes these top-rated recipes stand out.\n",
    "\n",
    "We acknowledge that there is ongoing debate about what constitutes a \"good\" recipe, but we chose to focus on the highest rating of 5 stars. Our goal was to uncover patterns and insights that distinguish these exceptional recipes. Through the Perceptron model, we aimed to discover the key factors that contribute to a recipe's top rating and use this knowledge to develop new recipes with a higher likelihood of achieving the coveted 5-star status.\n",
    "\n",
    "While our initial attempts to predict star ratings fell short, we remain hopeful that this new approach will shed light on the secrets of highly rated recipes. By analyzing the factors that set apart 5-star recipes, we aim to gain a deeper understanding of what makes a recipe truly outstanding. Let's delve into the Perceptron model and explore the world of exceptional recipes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def build_perceptron_model(df):\n",
    "    #df = pd.read_csv('test123.csv')\n",
    "\n",
    "    # Save the 'Rating' column separately\n",
    "    ratings = df['Rating']\n",
    "    \n",
    "    # Drop the 'Rating' column from the dataframe\n",
    "    df = df.drop(['Rating'], axis=1)\n",
    "    df = df.drop(['Rating_Count'], axis=1)\n",
    "    df = df.select_dtypes(include=['int', 'float'])\n",
    "    # Get the column names of the dataframe\n",
    "    col_names = df.columns\n",
    "\n",
    "    # Convert the dataframe to a numpy array\n",
    "    X = df.select_dtypes(include=['int', 'float']).values\n",
    "\n",
    "    # Define the target variable\n",
    "    y = (ratings > 4.5).astype(int).values\n",
    "\n",
    "    # Verify that X and y have the same length\n",
    "    assert len(X) == len(y), \"X and y must have the same length\"\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the Perceptron model to the training data\n",
    "    model = Perceptron()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Compute the predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Get the weights learned by the perceptron\n",
    "    weights = model.coef_[0]\n",
    "\n",
    "    # Create a new dataframe with the column names and weights\n",
    "    weights_df = pd.DataFrame({'column': col_names, 'weight': weights})\n",
    "\n",
    "    # Print the weights dataframe\n",
    "    print(weights_df)\n",
    "\n",
    "    #save_df(weights_df,'FML.csv')\n",
    "    return weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_data = build_perceptron_model(Updated_Data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were thrilled with the remarkable result we achieved using the Perceptron model: an impressive accuracy score of 0.95. It seemed almost too good to be true, so we decided to put it to the test. We selected 20 recipes from a different website that were not part of our original dataset, and to our delight, the model accurately predicted whether these recipes would be rated as 5 stars or not. This outcome reinforced our confidence in the model's ability to identify exceptional recipes and opened up exciting possibilities for future applications and enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = weights_data.sort_values(by='weight')\n",
    "\n",
    "# printing the top 10 rows\n",
    "print(sorted_df.tail(10)[['column', 'weight']])\n",
    "\n",
    "# printing the bottom 10 rows\n",
    "print(sorted_df.head(10)[['column', 'weight']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the weights obtained from our model, we identified the top 5 factors that contribute the most to a recipe being rated as 5 stars. These factors include:\n",
    "prep time,cook time,servings,calories,number of ingredients and number of steps.\n",
    "\n",
    "These variables had the highest weights in our weights dataframe, indicating their significant influence on the rating. Conversely, we also identified the 5 worst ingredients, which included popularity score, prep, cook, total time, and servings. These factors had negative weights, suggesting that recipes with longer preparation and cooking times, higher total time, and larger servings are less likely to receive a 5-star rating.\n",
    "\n",
    "These findings provide valuable insights into the preferences of users when it comes to recipe ratings. Recipes with shorter cooking times, lower servings (indicating higher calories per serving), and unique ingredients tend to have a higher likelihood of receiving a 5-star rating. Additionally, the negative weight associated with popularity score suggests that recipes with lower popularity scores may be more likely to receive higher ratings. This supports our hypothesis that incorporating rare ingredients such as caviar or lobster could potentially improve the chances of a recipe being rated highly. Overall, these findings open up new possibilities for creating recipes that align with user preferences and have a higher probability of receiving favorable ratings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to conclude : bla bla bla shapira pls help :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize results? haha updated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

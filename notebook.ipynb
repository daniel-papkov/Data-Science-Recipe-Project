{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes a good recepie?\n",
    "first we need to import the revelevant libraries to start scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to start scraping we will first get all the receipie names and links and save them to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_page_thespruceeats():#this returns a list of all the links to receipies on the page:\n",
    "    # URL to scrape\n",
    "    url = \"https://www.thespruceeats.com/search?q=&searchType=recipe\"\n",
    "\n",
    "    # Configure the Selenium webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run in headless mode (no GUI)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    # Get the page source and parse it with BeautifulSoup\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    results_div = soup.find(\"div\", attrs={\"class\": \"results-list__container\"})\n",
    "    recipe_names = []\n",
    "    recipe_links = []\n",
    "\n",
    "    # Scrape the first page\n",
    "    for li in results_div.find_all(\"li\", class_=\"results__item\"):\n",
    "        if li.find(\"a\") is not None:\n",
    "            link = li.find(\"a\").get(\"href\")\n",
    "        else:\n",
    "            link = ''\n",
    "\n",
    "        if li.find(\"h4\", class_=\"card__title\") is not None:\n",
    "            name = li.find(\"h4\", class_=\"card__title\").text.strip()\n",
    "        else:\n",
    "            name = ''\n",
    "        \n",
    "        recipe_names.append(name)\n",
    "        recipe_links.append(link)\n",
    "\n",
    "    # Scrape subsequent pages if the \"Next\" button exists\n",
    "    while True:\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".pagination__item-link--next\")))\n",
    "            next_button.click()\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Get the page source and parse it with BeautifulSoup\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "            results_div = soup.find(\"div\", attrs={\"class\": \"results-list__container\"})\n",
    "\n",
    "            # Scrape recipe names and links\n",
    "            for li in results_div.find_all(\"li\", class_=\"results__item\"):\n",
    "                if li.find(\"a\") is not None:\n",
    "                    link = li.find(\"a\").get(\"href\")\n",
    "                else:\n",
    "                    link = ''\n",
    "\n",
    "                if li.find(\"h4\", class_=\"card__title\") is not None:\n",
    "                    name = li.find(\"h4\", class_=\"card__title\").text.strip()\n",
    "                else:\n",
    "                    name = ''\n",
    "                print(f\"{name} added\")\n",
    "                recipe_names.append(name)\n",
    "                recipe_links.append(link)\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame with the recipe names and links\n",
    "    df = pd.DataFrame({\"Recipe_name\": recipe_names, \"Recipe_link\": recipe_links})\n",
    "\n",
    "    # Write DataFrame to a CSV file\n",
    "    df.to_csv(\"Recipe_Links_and_Names.csv\", index=False)\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets run this function!. we save this to a csv to speed up work later as we are no longer depenadant on the webdriver in another place and also in case of internet lose during the next tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pepper Steak Stir-Fry Recipe added\n",
      "Grilled Salmon Burgers With Radicchio Slaw and Sambal Mayonnaise added\n",
      "Kentucky Buck Cocktail Recipe added\n",
      "Sparkling Borage Cocktail added\n",
      "Lunch Box-Worthy Falafel Kebabs added\n",
      "Chipotle Pumpkin Queso Dip added\n",
      "Flamin' Hot Cheetos Mac and Cheese Bites added\n",
      "Cherry Vinegar Recipe added\n",
      "Turkish Ramadan Flat Bread (Pide) added\n",
      "Carrot, Cabbage, and Kohlrabi Slaw With Miso Dressing Recipe added\n",
      "Esquites Recipe (Mexican Corn Off the Cob) added\n",
      "Gillie Fix Cocktail Recipe added\n",
      "Raspberry Snakebite Recipe added\n",
      "Stuffed Italian Sourdough Loaf Recipe added\n",
      "Strawberry Chicken Salad With Champagne Vinaigrette Recipe added\n",
      "Lasagne All'astice (Lobster Lasagna) Recipe added\n",
      "Pear and Pomegranate Champagne Shrub Recipe added\n",
      "Homemade Smoked Maple Bacon added\n",
      "Baked S’mores Skillet Dip Recipe added\n",
      "Garlic Chicken Primavera Pasta added\n",
      "Cold Soba Noodle Salad Recipe added\n",
      "Vegetarian Tofu Tacos added\n",
      "Bay Hill Hummer added\n",
      "A Recipe for Risotto Made With Amarone Wine (Risotto all'Amarone) added\n",
      "Cranberry Orange Bread added\n",
      "Tropical Raspberry Smoothie Recipe added\n",
      "Mexican Lasagna added\n",
      "Grilled Prosciutto Wrapped Pork Chops added\n",
      "Cookie Dough Pops added\n",
      "Kentucky Coffee added\n",
      "Irish Ale Cocktail added\n",
      "Gluten-Free Fadge (Irish Potato Rolls) added\n",
      "Spring Greens: Beans With Lemon Ginger Butter Recipe added\n",
      "Super-Easy Balsamic Roasted Strawberry added\n",
      "Easy Classic Hollandaise Sauce With Tarragon added\n",
      "Easy Roasted Rhubarb Recipe added\n",
      "Traditional Smoked Mackerel Fishcakes added\n",
      "Pork and Onion Meatballs With Tomato Sauce added\n",
      "Easy and Traditional Blackberry Jelly added\n",
      "Yorkshire Ginger Parkin Biscuits added\n",
      "Easy Classic British Cheese Scones added\n",
      "Creamed Leeks and Smoked Haddock Recipe added\n",
      "Christmas Stollen Recipe added\n",
      "Anglesey Egg Recipe added\n",
      "White Bread (Made With Condensed Milk) Recipe added\n",
      "Garlic Chive Butter added\n",
      "Mint Butter Recipe added\n",
      "Easy Classic Elderflower Cordial added\n",
      "Welsh Leek and Stilton Soup added\n",
      "Super Apple and Almond Tart from Brendan Lynch added\n",
      "Venison Shepherd's Pie added\n",
      "Golden Beetroot Pasta added\n",
      "Gooseberry Compote added\n",
      "Bumbleberry Crumb Bars added\n",
      "Orange Sage Bread added\n",
      "World War I Era White Bread Recipe added\n",
      "Crispy Quinoa With Kale added\n",
      "Quinoa With Pasta Sauce and Cheese added\n",
      "Artichoke Gruyere Dip added\n",
      "The Recipe for Cheese Crouton added\n",
      "Peppermint Chocolate Spoons added\n",
      "Strawberry Pâte de Fruits Recipe added\n",
      "Tuna Ceviche Recipe added\n",
      "Matcha Green Tea Truffles added\n",
      "Keto Truffles added\n",
      "Chocolate-Dipped Plums added\n",
      "Red-White-and-Blue Cake Balls Recipe added\n",
      "Red, White, and Blue Gummies Recipe added\n",
      "Pumpkin Marshmallows added\n",
      "Basil Lemonade Recipe added\n",
      "Funfetti Lollipops added\n",
      "Chocolate Coconut Cream Patties added\n",
      "Tasty Nutella Cake Pops added\n",
      "Chocolate Firecrackers added\n",
      "Gluten-Free Chicken and Dumplings added\n",
      "Keto Angel Food Cake added\n",
      "Double Chocolate Chip Cookies Recipe added\n",
      "Keto Chocolate Cake Recipe added\n",
      "Reindeer Chow Recipe added\n",
      "Nutella Nougat added\n",
      "Meringue Rose Pops Recipe added\n",
      "Woodson’s Confidence “Cocktale” added\n",
      "Keto Queso Dip Recipe added\n",
      "Strawberries and Cream Cookies added\n",
      "Rose Jam added\n",
      "Sweet Potato Tacos added\n",
      "Kimchi Pickled Eggs Recipe added\n",
      "Tuna Tartare Recipe added\n",
      "Vegan Instant Pot Cashew Yogurt Recipe added\n",
      "Brown Butter Cookies added\n",
      "Pineapple Cheese Ball Recipe added\n",
      "Instant Pot Cauliflower added\n",
      "Sour Cream Coffee Cake Muffins added\n",
      "Keto Fat Bombs added\n",
      "Creamy Potato Salad added\n",
      "Eight Treasure Rice Pudding added\n",
      "Bubble Bread added\n",
      "Thai Grilled Pork Salad added\n",
      "Baked Turkey Wings Recipe added\n",
      "Snickerdoodle Cupcakes Recipe added\n",
      "Air Fryer Chicken Tenders Recipe added\n",
      "Italian Margarita Recipe added\n",
      "Thai Shrimp Salad (Shrimp Yum Goong) added\n",
      "Lyonnaise Salad added\n",
      "Tom Yum Fried Rice added\n",
      "Couscous Belboula (Barley Couscous) Recipe added\n",
      "Instant Pot Breakfast Casserole Recipe added\n",
      "Edible Cookie Dough added\n",
      "Crock Pot Sweet and Spicy Meatballs added\n",
      "Keto \"Apple\" Crisp Recipe added\n",
      "Three-Ingredient Swirled Nutella Fudge added\n",
      "Lavender Fudge added\n",
      "Keto Crab Cakes Recipe added\n",
      "Sugar-Free Chocolate Nut Clusters added\n",
      "Mango and Ginger Kombucha Mule With Tequila added\n",
      "Christmas Tree Pretzel Rods added\n",
      "Haupia Pie Recipe added\n",
      "Classic Butterscotch Drops Recipe added\n",
      "Bacon-Wrapped Jalapeño Poppers added\n",
      "Chili Verde Recipe added\n",
      "Mazurek Królewski: Polish Royal Mazurek Recipe added\n",
      "Roasted Carrots and Parsnips With Fresh Herbs Recipe added\n",
      "Czech Fried Cheese (Syr Smazeny) added\n",
      "Traditional Peach Butter added\n",
      "Blueberry-Filled Pierogi added\n",
      "Pierogi and Naleśniki Meat Filling Recipe added\n",
      "Plum Pierogi Filling added\n",
      "Fast Limoncello added\n",
      "Easy Rhubarb Compote added\n",
      "Elderflower Syrup added\n",
      "Potatoes Romanoff added\n",
      "Kentucky Bourbon Sauce Barbeque added\n",
      "Best Baked Beans Recipe added\n",
      "Creamy Corn Custard added\n",
      "Polish Sauerkraut Soup (Kapusniak) added\n",
      "Romanian Sausages (Mititei) added\n",
      "Romanian Pork Cordon Bleu Schnitzels Recipe added\n",
      "Polish Potato Salad (Sałatka Kartofli) added\n",
      "Serbian Coleslaw Recipe added\n",
      "Seared Scallops and Shrimp With Balsamic Strawberries added\n",
      "Classic French Steamed Mussels added\n",
      "Lemongrass Ginger Sauce added\n",
      "Classic Pan Sauce for Fish added\n",
      "Easy Classic Duck Confit Recipe added\n",
      "Gingerbread Cookies - Czech Pernik na Figurky added\n",
      "Quick Serbian Kajmak Recipe added\n",
      "Traditional Dutch Taai-Taai Cookie added\n",
      "Stroopkoeken (Dutch Caramel Cookies) Recipe added\n",
      "Pulled Pork Croquettes added\n",
      "Groninger Mustard Soup added\n",
      "Ukrainian Meatless Beet Soup (Borshch) added\n",
      "Bulgarian Fish Roe Appetizer - Tarama added\n",
      "Orange Mint Tea added\n",
      "Easy Almond Bundt Cake Recipe With Almond-Flavored Icing added\n",
      "Low-Sugar Blueberry-Rhubarb Jam Recipe added\n",
      "Polish Wafle Wafer Cookies added\n",
      "Polish Strawberry Kissel added\n",
      "Grilled Shrimp and Scallops Skewers (Uncle Mike's Spice Mix) added\n",
      "Rhubarb BBQ Sauce Recipe added\n",
      "Dairy Free Carrot Cake Recipe added\n",
      "Dairy-Free Gluten-Free Banana Bread for Celiacs added\n",
      "Dairy-Free White Cake added\n",
      "Vegan Buttercream Frosting added\n",
      "Vegan Buttercream Frosting Recipe added\n",
      "Seafood a la King With Tilapia, Shrimp, and Crab added\n",
      "Cajun Perch Po'Boys Recipe added\n",
      "Fish and Seafood, Beer Batter Style added\n",
      "Low-Sugar Spiced Plum Jam Recipe added\n",
      "Vegan Pumpkin Cupcakes added\n",
      "Vegan No Bake Cookies added\n",
      "Dairy-Free Beef Stroganoff added\n",
      "Vegetarian Yellow Split Pea Soup added\n",
      "Dairy-Free Basil Cashew Pesto: Savory Sauces added\n",
      "Homemade Cashew Butter for Vegans added\n",
      "Dairy-Free Chocolate Covered Pretzels added\n",
      "Dairy-Free Gingerbread People added\n",
      "Dairy-Free Vegan Whoopie Pies added\n",
      "Fresh Peach Topping for Ice Cream added\n",
      "Croatian Sour Cherry Strudel (Štrudlu s Višnjama) added\n",
      "Authentic Croatian Bajadera Torte Recipe added\n",
      "Polish Smoked Sausage and Sauerkraut added\n",
      "Polish Mushroom Pierogi and Naleśniki Filling Recipe added\n",
      "Polish Open-Faced Sandwich (Zapiekanka) added\n",
      "Day-Old Bread Gets New Life in German Bread Dumplings added\n",
      "German Mashed Apples and Potatoes added\n",
      "Gluten-Free Country Peach Cobbler Recipe added\n",
      "Homemade Gluten-Free Corn Tortilla added\n",
      "Gluten-Free Cheese Sauce added\n",
      "Galaktoboureko: Custard Pie With Phyllo added\n",
      "Easy 4-Ingredient Baked Brisket added\n",
      "Layered Strawberry Dessert With Ladyfingers and Cream added\n",
      "German Fresh Fruit Torte added\n",
      "Gluten-Free Apple Oat Muffins added\n",
      "Chocolate Raspberry Tart added\n",
      "Carrot Cake Ice Cream added\n",
      "Strawberries With Balsamic Black Pepper added\n",
      "Herbed Vegetable Soup added\n",
      "Gluten-Free Nacho Cheese Dip added\n",
      "Gluten-Free Apple Pie Recipe added\n",
      "Southern-Style Gluten-Free Banana Meringue Pudding added\n",
      "Gluten-Free Caramel Corn added\n",
      "Tsoureki (Greek Easter Bread) Recipe added\n",
      "Kolokithia Yemista Recipe (Papoutsakia-Style) added\n",
      "Succulent Glazes and Rubs for Cornish Game Hens added\n",
      "Roasted Cod With Tomatoes, Basil, and Mozzarella added\n",
      "Mexican Flag Shooter (Bandera Mexicana) added\n",
      "How To Make Miso Marinade added\n",
      "Pasta With Mixed Seafood (Pasta alla Posillipo) added\n",
      "Classic French Winter Vegetable Ragout added\n",
      "German Pound Cake with Lemon and Vanilla Recipe added\n",
      "German Cheesecake With Quark (Kaesekuchen) added\n",
      "German Mini Nut Bars (Nussecken) added\n",
      "Austrian Pancakes With Raisins (Kaiserschmarrn) added\n",
      "Make Your Own Refreshing Watermelon Martinis added\n",
      "Penne Pasta With Fresh Artichokes Recipe added\n",
      "Moshari Kokkinisto or Reddened Beef Stew Recipe added\n",
      "Ariani: A Refreshing Yogurt Beverage added\n",
      "Lamb Meatballs With Tzatziki Sauce added\n",
      "French Macaroni and Cheese Recipe added\n",
      "French Pork Medallions and Caramelized Apples added\n",
      "Classic French Canadian Venison Tourtiere added\n",
      "Traditional French Scallops Sage Cream added\n",
      "Black Forest Bread - Schwarzwaelder Kruste added\n",
      "German Lentil Stew With Noodles and Frankfurters added\n",
      "German Flädlesuppe added\n",
      "German Yogurt Salad Dressing added\n",
      "Greek Fisherman's Soup Recipe (Kakavia) added\n",
      "Psarosoupa (Fish Soup With Red Snapper and Vegetables) Recipe added\n",
      "French Crepes With Salted Butter Caramel added\n",
      "Jambon Beurre (Ham Butter Sandwich) Recipe added\n",
      "French Strawberry and Vanilla Charlotte Dessert added\n",
      "Pennsylvania Dutch Low-Sugar Apple Butter added\n",
      "German Osterbrot Easter Bread added\n",
      "Easy Baguette (Stangenbrot) Bread added\n",
      "Recipe for Rye Bread with Sourdough (Roggenbrot) added\n",
      "Whole Rye Bread Recipe added\n",
      "Streuseltaler German Pastry Recipe added\n",
      "Salmon With Citrus-Balsamic Vinaigrette added\n",
      "Brazilian Carrot Cake - Bolo de Cenoura Com Cobertura de Chocolate added\n",
      "Hallullas: Chilean \"Biscuits\" added\n",
      "Bien Me Sabe - Venezuelan Coconut Cream Cake added\n",
      "Bolo de Fuba (Brazilian Cornmeal Cake) Recipe added\n",
      "Easy Baked Ham With Pineapple added\n",
      "Crockpot Short Ribs and Rice added\n",
      "Apple Crisp With Blueberries and Raspberries added\n",
      "Classic Shortbread added\n",
      "Chicken and Ham Pasta Bake Recipe added\n",
      "Chicken and Orzo Bake added\n",
      "Chicken Salad With Walnuts and Grapes Recipe added\n",
      "Chicken and Rice Salad added\n",
      "Cranberry Pumpkin Seed Muffins added\n",
      "Seafood Casserole With Rice or Pasta added\n",
      "Slow Cooker Chicken With Honey-Hoisin Sauce Recipe added\n",
      "Slow Cooker Chicken and Shrimp With Fettuccine added\n",
      "Brown Sugar and Cinnamon Funnel Cakes added\n",
      "Shrimp and Crab Gumbo added\n",
      "Pepper and Garlic Eye of Round Roast Recipe added\n",
      "Roast Beef With Garlic and Thyme Recipe added\n",
      "Pancakes With Strawberry Sauce Recipe added\n",
      "Arroz al Horno, an Oven-Baked Spanish Rice Recipe added\n",
      "Bacalao con Tomate Recipe added\n",
      "Noodle Paella Recipe: Fideuà added\n",
      "Spanish Roast Lamb (Cordero Asado or Lechazo) added\n",
      "Ham, Cheese, and Chorizo Appetizer With Bread added\n",
      "Patatas Alioli: Alioli Potatoes added\n",
      "Spanish Crayfish in Tomato Sauce added\n",
      "Shrimp With Mushrooms, Garlic and Wine Tapa added\n",
      "Spanish Potatoes au Gratin (Patatas Gratinadas) added\n",
      "Make Fried Cauliflower...Spanish Style added\n",
      "Puré de Patatas con Ajo y Pimenton (Garlic Paprika Mashed Potatoes) added\n",
      "Turkey Bacon Ranch Sandwich added\n",
      "Thanksgiving Leftovers Party Buns added\n",
      "Chicken Po'boy Sandwich Recipe added\n",
      "Korean Barbecue Steamed Buns With Bulgogi Beef Recipe added\n",
      "Chorizo and Potato Empanadas added\n",
      "Southern Style Sweet Fruit Tea Three Ways added\n",
      "Cabbage Casserole With Cheddar Cheese added\n",
      "Skillet Buttermilk Cornbread With Corn added\n",
      "Cheese and Bacon Cornbread added\n",
      "Crock Pot Swiss Chicken Casserole added\n",
      "Eggplant and Tomato Casserole With Mozzarella Cheese added\n",
      "Green Tomato Cake With Nuts added\n",
      "Quick Ham and Noodle Casserole added\n",
      "Baked Mac and Cheese With Ham and Broccoli added\n",
      "Spinach and Beef Lasagna With Ricotta Cheese added\n",
      "English Pea Pesto Pasta With Ham added\n",
      "Vanilla Wafer Cookie Crumb Crust added\n",
      "Sweet Onion Salad Dressing added\n",
      "Quick Pesto Roasted Chicken Recipe added\n",
      "Round Steak and Gravy for Supper added\n",
      "Spanish Chicken in Orange Sauce added\n",
      "Spanish Eggs in Purgatory added\n",
      "Three Green Chile Meatloaf Recipe added\n",
      "Easy Poblano Pepper Relish added\n",
      "Slow Cooker Feijoada (Brazilian Black Bean Stew) Recipe added\n",
      "Torta de Fiambre: Urugayan Ham and Cheese Tart added\n",
      "Cazuela De Camarón: Shrimp in Plantain and Peanut Sauce added\n",
      "Milanesa de Carne Recipe added\n",
      "Chimichurri Chicken en Croute Recipe added\n",
      "Texas Caviar Black-Eyed Pea Dip With Jalapeño Peppers added\n",
      "Basic Black Eyed Peas added\n",
      "Fudgy Chocolate Chip Walnut Brownies Recipe added\n",
      "Crunchy Pistachio Brittle added\n",
      "Southern Chocolate Chess Pie added\n",
      "Chocolate Cream Pie With Whipped Cream added\n",
      "How to Make No-Bake Chocolate Cream Pie added\n",
      "Candy Cane Cookies added\n",
      "Blueberry Muffin Mix in a Jar Recipe added\n",
      "Sweet and Spicy Crock Pot Cocktail Franks added\n",
      "Crockpot Barbecue Beef Sandwiches added\n",
      "Slow Cooker Chili With Taco Seasoning added\n",
      "Sausage Bites in Sweet and Tangy Barbecue Sauce added\n",
      "Crockpot Kielbasa With Cabbage and Potatoes added\n",
      "Balsamic Marinated Chicken Breasts added\n",
      "Green Tomato Soup With Country Ham Recipe added\n",
      "Grilled Pork Chops With Balsamic Marinade added\n",
      "Louisiana Grillades and Grits added\n",
      "Whole Grilled Onions added\n",
      "Easy Meatloaf Sliders added\n",
      "Classic Two Egg Vanilla Pound Cake added\n",
      "4-Ingredient Crock-Pot Pork Barbecue added\n",
      "Pumpkin Oat Muffins With Dates Recipe added\n",
      "Pulled Pork With Peppers added\n",
      "Salsa Rosa: A Creamy Spanish Cocktail Sauce added\n",
      "Spanish Champiñones, Pimientos y Ajo (Mushrooms, Peppers and Garlic) added\n",
      "Sopa de Lentejas: Spanish Lentil Soup added\n",
      "Spanish Vegetable Soup (Menestra de Verduras) added\n",
      "Hamburger Soup With Black-Eyed Peas and Kale added\n",
      "Homemade Pumpernickel Bread added\n",
      "Easy Egg-Free Chocolate Mousse added\n",
      "Slow Cooker Red Beans With Ham and Rice added\n",
      "Crock Pot Red Beans and Rice With Ham added\n",
      "Beer-Battered Chicken Strips added\n",
      "Appetizer Hot Dogs in Barbecue Sauce added\n",
      "Old-Fashioned Chocolate Ice Cream Soda added\n",
      "Pecan Praline Ice Cream Recipe added\n",
      "Baked Pork Tenderloin added\n",
      "Holiday Potato Casserole added\n",
      "Smashed Red Potatoes With Garlic added\n",
      "Loaded Potato Casserole added\n",
      "Roasted Split Turkey Breast With Cajun Spices added\n",
      "Easy Chocolate Zucchini Cake Recipe added\n",
      "Zucchini Patties With Parmesan Cheese added\n",
      "Zucchini Casserole With Tomato Sauce and Bacon Recipe added\n",
      "How to Cook Cracked Hominy: Maíz Trillado added\n",
      "Brazilian Risoles (Chicken and Cheese Croquettes) added\n",
      "Stuffed Yuca Balls - Yuquitas Rellenas added\n",
      "Empanadas de Pizza added\n",
      "Golden Apricot Nut Bread added\n",
      "Apple Crunch Dessert With Cinnamon added\n",
      "Pineapple Delight Bars Recipe added\n",
      "Open-Faced Reuben Burgers added\n",
      "Classic Reuben Sandwich added\n",
      "Homemade Crock Pot Barbecue Meatballs added\n",
      "Cocktail Meatballs With Apricot Preserves added\n",
      "Slow Cooker Honey Glazed Ham added\n",
      "Moist Banana Cupcakes With Vanilla Icing added\n",
      "Deviled Eggs With Mayonnaise Recipe added\n",
      "Kentucky Burgoo added\n",
      "Slow Cooker Lamb Chops With Tomatoes and Garlic added\n"
     ]
    }
   ],
   "source": [
    "get_full_page_thespruceeats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have the receipie links we need to scrape each one we will break this into steps as much as possible first we need to soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_soup_object(url):\n",
    "    ###\n",
    "    #url = \"https://www.thespruceeats.com/search?q=&searchType=recipe\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    return soup\n",
    "    ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we will gather information by the order it is represented in our site using functions because we will loop over it soon!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our first function will gather the cook time prep time total time to cook and the amount of servings\n",
    "this was very messy because of the formating can change widly from one recepie to another time could look like 25 min or 2h 15 min or 2 hours 10 min or countless other variations that why we used a complex regex expression to combat this issue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cook_times(soup_obj):\n",
    "    lines=[]\n",
    "    results_items = soup_obj\n",
    "    results_items = soup_obj.find_all(class_='comp article__decision-block mntl-block')\n",
    "    if(results_items==[]):\n",
    "        soup = soup_obj\n",
    "        results_items = soup.find_all(class_='comp project-meta')        \n",
    "\n",
    "    for item in results_items:\n",
    "        item.find_all(class_='meta-text__data')\n",
    "        for sub_item in item:\n",
    "            if bool(sub_item.text.strip()):\n",
    "                clean_text = sub_item.text.strip().replace('\\n', '')\n",
    "                lines.append(clean_text)\n",
    "\n",
    "    if(len(lines)>1):\n",
    "        new_string = lines[0] + lines[1]\n",
    "        lines[0]= new_string\n",
    "\n",
    "    #use regex expressions to clean up the line we get it looks something like this\n",
    "    #['Prep: 15 minsCook: 20 minsTotal: 35 minsServings: 6 servingsYield: 1 cake', 'ratingsAdd a comment']\n",
    "    #prep = re.findall(r'Prep:\\s*(\\d+)\\s*mins', lines[0])[0]\n",
    "    cook_time_str = re.findall(r'Cook:\\s*(?:(\\d+)\\s*(?:hrs?|hours?)\\s*)?(?:(\\d+)\\s*mins?)?', lines[0])[0]\n",
    "    prep_time_str = re.findall(r'Prep:\\s*(?:(\\d+)\\s*(?:hrs?|hours?)\\s*)?(?:(\\d+)\\s*mins?)?', lines[0])[0]    \n",
    "    total_time_str = re.findall(r'Total:\\s*(?:(\\d+)\\s*(?:hrs?|hours?)\\s*)?(?:(\\d+)\\s*mins?)?', lines[0])[0]\n",
    "# Convert cook time to minutes\n",
    "\n",
    "\n",
    "    hours = int(cook_time_str[0]) if cook_time_str[0] else 0\n",
    "    minutes = int(cook_time_str[1]) if cook_time_str[1] else 0\n",
    "    cook_time_minutes = hours * 60 + minutes\n",
    "\n",
    "    hours = int(prep_time_str[0]) if prep_time_str[0] else 0\n",
    "    minutes = int(prep_time_str[1]) if prep_time_str[1] else 0\n",
    "    prep_time_minutes = hours * 60 + minutes\n",
    "\n",
    "\n",
    "    hours = int(total_time_str[0]) if total_time_str[0] else 0\n",
    "    minutes = int(total_time_str[1]) if total_time_str[1] else 0\n",
    "    total_minutes = hours * 60 + minutes\n",
    "    #total = re.findall(r'Total:\\s*(\\d+)\\s*mins', lines[0])[0]\n",
    "    #servings = re.findall(r'Servings?:\\s*(\\d+?)\\s*(?:to\\s*\\d+)?\\s*(?:servings|ratings)', lines[0])[0] # sometimes instead of saying servings 6 they say servings 6 to 8 in this case we make it servings 6\n",
    "    #servings = re.findall(r'servings?:\\s*(\\d+?)\\s*(?:to\\s*\\d+)?\\s*(?:servings?|ratings)', lines[0], re.IGNORECASE)[0]\n",
    "    #text = \"The serving size is 3 servings per container.\"\n",
    "    \n",
    "    if(lines[0].count('serv')):\n",
    "        match = re.search(r'serv\\w*:\\D*(\\d+)', lines[0], re.IGNORECASE)\n",
    "        if match:\n",
    "            servings=(match.group(1))\n",
    "    else:\n",
    "        servings=1\n",
    "\n",
    "    \n",
    "\n",
    "    # Create a pandas DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Prep': [prep_time_minutes],\n",
    "        'Cook': [cook_time_minutes],\n",
    "        'Total': [total_minutes],\n",
    "        'Servings': [servings]\n",
    "    })\n",
    "    df = df.astype(int)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now for the second block of information which contains the rating for the dish \n",
    "this one was also messy because instead of writing a number they only display the star rating with 0.5 increments so we needed to count how many full stars and half stars there are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stars(soup_obj):    \n",
    "    soup = soup_obj\n",
    "    results_items = soup.find_all(class_='comp js-feedback-trigger aggregate-star-rating mntl-block')    \n",
    "    #print(results_items.prettify())\n",
    "    for item in results_items:##result items size is 1\n",
    "        text=item.prettify()\n",
    "        full_stars=text.count('class=\"active\"')\n",
    "        half_stars=text.count('class=\"half\"')\n",
    "        return(full_stars+0.5*half_stars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up next is the rating count nothing to special about this one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_count(soup_obj):\n",
    "    soup = soup_obj\n",
    "    rating_elements = soup.find_all(\"div\", attrs={'class': \"comp aggregate-star-rating__count mntl-aggregate-rating mntl-text-block\"})\n",
    "    for rating_element in rating_elements:\n",
    "        rating_text = rating_element.text.strip()\n",
    "        try:\n",
    "            num_ratings = int(rating_text.split()[0])\n",
    "            return num_ratings\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up next we have the 3rd block of information which gives us the nutritional values there was an issue here that alcoholic beverages had no nutrinal value and the site had a lot of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nutritional_values(soup_obj):\n",
    "    soup = soup_obj\n",
    "    results_items = soup.find_all(class_='nutrition-info__table--row')\n",
    "\n",
    "    nutritional_vals=[]    \n",
    "\n",
    "\n",
    "    for item in results_items:\n",
    "        nutritional_vals.append(item.text.strip())\n",
    "    new_list = []\n",
    "    for s in nutritional_vals:\n",
    "        # Split the string by the \\n character and add the two parts to a new list\n",
    "        parts = s.split('\\n')\n",
    "        # Add the new strings to the new list in the desired format\n",
    "        #[caleories:934,fat:134g,carbs:999]\n",
    "        new_list.extend([parts[1], parts[0]])\n",
    "    #[calories,934,far,134g,carbs,1123,]\n",
    "    df = pd.DataFrame({'nutrient': new_list[::2], 'value': new_list[1::2]})\n",
    "\n",
    "    # Set 'nutrient' column as index and transpose DataFrame\n",
    "    df = df.set_index('nutrient').T\n",
    "    if df.empty: #recepies like cocktails have no calories\n",
    "\n",
    "        df = pd.DataFrame(columns=['nutrient', 'Calories', 'Fat', 'Carbs', 'Protein'])\n",
    "\n",
    "        # add a row filled with zeros\n",
    "        df.loc[0] = ['value', 0, '0g', '0g', '0g']\n",
    "        df['Calories'] = df['Calories'].astype(np.int32)\n",
    "        return df \n",
    "\n",
    "    df['Calories'] = df['Calories'].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next is the 4th block of relevant information which holds the list of ingridients this was also not fun as the class names for some receipies changed and werent consistent resulting in a lot of hotfixes and hours of debugging which is why we added cond to check if its the old varient of the site or not and if its the new one we can sometimes filter the ingridients from the start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingridients(soup_obj):\n",
    "    cond=0\n",
    "    soup = soup_obj\n",
    "    span_elements = soup.find_all('span', {'data-ingredient-name': 'true'})\n",
    "\n",
    "    # create an empty list to store the ingredient names\n",
    "    ingredient_names = []\n",
    "\n",
    "    # loop over the span elements and extract their text content\n",
    "    for span in span_elements:\n",
    "        ingredient_names.append(span.text)\n",
    "    #print(ingredient_names)\n",
    "    if(len(ingredient_names)>0):\n",
    "        return(ingredient_names)\n",
    "    else:\n",
    "        cond=0\n",
    "        soup = soup_obj\n",
    "        results_items = soup.find_all(class_='structured-ingredients__list text-passage')\n",
    "                                            #comp ingredient-list simple-list simple-list--bulleted  \n",
    "        #print(results_items)                                           \n",
    "        if(results_items==[]): #sometimes they like to change the class name\n",
    "            soup = soup_obj\n",
    "            results_items = soup.find_all(class_='simple-list__item js-checkbox-trigger ingredient text-passage')\n",
    "            cond=1\n",
    "        nutritional_vals=[]\n",
    "        if(results_items==[]):\n",
    "            return []\n",
    "        \n",
    "        final_lst=[]\n",
    "\n",
    "        for item in results_items:    \n",
    "            nutritional_vals.append(item.text.strip())\n",
    "            #print(item.text.strip())\n",
    "        if(cond==1):\n",
    "            return nutritional_vals\n",
    "        else:        \n",
    "            for i in nutritional_vals:\n",
    "                my_list = [s.strip() for s in i.split('\\n\\n\\n')]\n",
    "                final_lst.extend(my_list)\n",
    "                #print(final_lst)\n",
    "            \n",
    "            return(final_lst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also wanted to add some info of our own so we looked at the ingridients and tried to figure out if its dairy meat fur(parve) or dairy and meat as for keywords we just googled common meat or common dairy products and coppied the first few "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_recipe(ingredients):\n",
    "    dairy_keywords = [\"milk\", \"cheese\", \"yogurt\", \"cream\", \"butter\", \"whey\", \"casein\", \"curds\"]\n",
    "    meat_keywords = [\"beef\", \"chicken\", \"pork\", \"lamb\", \"turkey\", \"venison\", \"duck\", \"bacon\", \"sausage\",\n",
    "                     \"ham\", \"prosciutto\", \"pepperoni\", \"salami\", \"chorizo\", \"bresaola\", \"pastrami\",\n",
    "                     \"corned beef\", \"veal\", \"goose\", \"game\", \"elk\", \"bison\", \"rabbit\", \"boar\", \"guinea fowl\", \"quail\"]\n",
    "    \n",
    "    categories = {'Dairy': 0, 'Meat': 0, 'Fur': 1}\n",
    "    \n",
    "    for ingredient in ingredients:\n",
    "        ingredient = ingredient.lower()\n",
    "        if any(keyword in ingredient for keyword in dairy_keywords):\n",
    "            categories['Dairy'] = 1\n",
    "            categories['Fur'] = 0\n",
    "        elif any(keyword in ingredient for keyword in meat_keywords):\n",
    "            categories['Meat'] = 1\n",
    "            categories['Fur'] = 0\n",
    "            \n",
    "    df = pd.DataFrame(categories, index=[0])\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to merge all of this data into a single row of a dataframe and again we will be looping over this one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fast(url,recepie_name):\n",
    "    df = pd.DataFrame(columns=['Name','Prep', 'Cook', 'Total', 'Servings', 'Rating','Rating_Count','Dairy','Meat','Fur', 'Calories', 'Fat', 'Carbs', 'Protein', 'Ingredients'])\n",
    "    #Dairy\n",
    "    #Meat\n",
    "    #Fur\n",
    "    soup_obj= load_soup_object(url)\n",
    "\n",
    "    recipe_df = get_cook_times(soup_obj)\n",
    "\n",
    "    ratings_list = get_stars(soup_obj)\n",
    "\n",
    "    nutrition_df = get_nutritional_values(soup_obj)\n",
    "\n",
    "    ingredients = get_ingridients(soup_obj)\n",
    "\n",
    "    rating_num = get_rating_count(soup_obj)\n",
    "\n",
    "    meatdairy_df = analyze_recipe(ingredients)\n",
    "\n",
    "    if(ingredients==[]):\n",
    "        ingredients=['','','','']\n",
    "        print(type(nutrition_df['Calories'][0]))\n",
    "    new_row = {\n",
    "        'Name':recepie_name,\n",
    "        'Prep': recipe_df['Prep'][0],\n",
    "        'Cook': recipe_df['Cook'][0],\n",
    "        'Total': recipe_df['Total'][0],\n",
    "        'Servings': recipe_df['Servings'][0],\n",
    "        'Rating': ratings_list,\n",
    "        'Rating_Count':rating_num,\n",
    "        'Dairy':meatdairy_df['Dairy'][0],\n",
    "        'Meat':meatdairy_df['Meat'][0],\n",
    "        'Fur':meatdairy_df['Fur'][0],\n",
    "        'Calories': nutrition_df['Calories'][0],\n",
    "        'Fat': nutrition_df['Fat'][0],\n",
    "        'Carbs': nutrition_df['Carbs'][0],        \n",
    "        'Protein': nutrition_df['Protein'][0],\n",
    "        'Ingredients': [ingredients]\n",
    "    }\n",
    "    #print(new_row)\n",
    "\n",
    "    # add the new row to the DataFrame\n",
    "    df = pd.concat([df, pd.DataFrame(new_row)], ignore_index=True)\n",
    "    #df_concat = pd.concat([df1, df2], keys=['df2'])\n",
    "    df['Prep'] = df['Prep'].astype(float)\n",
    "    df['Cook'] = df['Cook'].astype(float)\n",
    "    df['Total'] = df['Total'].astype(float)\n",
    "    df['Servings'] = df['Servings'].astype(float)\n",
    "    df['Rating'] = df['Rating'    ].astype(float)\n",
    "    df['Calories'] = df['Calories'].astype(float)\n",
    "\n",
    "    # display the updated DataFrame\n",
    "    #print(\"another line added\")\n",
    "    return(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we want to do this for each receipe that was found up until now here comes another function :)\n",
    "it will take a while so again to speed up work later we are saving it to a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_scrape(csv_file_name):\n",
    "    recipe_names = []\n",
    "    recipe_links = []\n",
    "    count=0\n",
    "\n",
    "    with open(csv_file_name, encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            recipe = ','.join(row).strip().replace('\\x9c', '')  # Join recipe name and link with a comma, remove problematic characters\n",
    "            last_comma = recipe.rfind(',')  # Find the index of the last comma in the recipe string\n",
    "            if last_comma != -1:\n",
    "                recipe_name = recipe[:last_comma].strip()  # Get the recipe name before the last comma\n",
    "                recipe_link = recipe[last_comma+1:].strip()  # Get the recipe link after the last comma\n",
    "                recipe_names.append(recipe_name)\n",
    "                recipe_links.append(recipe_link)\n",
    "            else:\n",
    "                print(f\"Invalid row: {row}\")\n",
    "\n",
    "    final_df=pd.DataFrame()\n",
    "    for name, link in zip(recipe_names, recipe_links):\n",
    "        # print(f\"Recipe name: {name}\")\n",
    "        # print(f\"Recipe link: {link}\")\n",
    "        temp_df=merge_fast(link,name)\n",
    "        temp_df.set_index('Name', inplace=True)  # set the index of temp_df to the recipe name\n",
    "        if(final_df.empty):\n",
    "            final_df=temp_df\n",
    "        else:\n",
    "            final_df = pd.concat([final_df,temp_df])\n",
    "        print(final_df)\n",
    "\n",
    "    #final_df.to_pickle('dataframe.pkl')\n",
    "    #final_df.to_pickle('dataframe.pkl', protocol=4, encoding='utf-8')\n",
    "    final_df.to_csv('my_data.csv', index=True, encoding='utf-8')\n",
    "    #print(final_df)\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL 'Recipe_link': No scheme supplied. Perhaps you meant http://Recipe_link?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m fast_scrape(\u001b[39m'\u001b[39;49m\u001b[39mRecipe_Links_and_Names.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[12], line 23\u001b[0m, in \u001b[0;36mfast_scrape\u001b[1;34m(csv_file_name)\u001b[0m\n\u001b[0;32m     19\u001b[0m final_df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m name, link \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(recipe_names, recipe_links):\n\u001b[0;32m     21\u001b[0m     \u001b[39m# print(f\"Recipe name: {name}\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[39m# print(f\"Recipe link: {link}\")\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     temp_df\u001b[39m=\u001b[39mmerge_fast(link,name)\n\u001b[0;32m     24\u001b[0m     temp_df\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# set the index of temp_df to the recipe name\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m(final_df\u001b[39m.\u001b[39mempty):\n",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m, in \u001b[0;36mmerge_fast\u001b[1;34m(url, recepie_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mPrep\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCook\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTotal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mServings\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRating\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mRating_Count\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mDairy\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mMeat\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mFur\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCalories\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCarbs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mProtein\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIngredients\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[39m#Dairy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#Meat\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#Fur\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m soup_obj\u001b[39m=\u001b[39m load_soup_object(url)\n\u001b[0;32m      8\u001b[0m recipe_df \u001b[39m=\u001b[39m get_cook_times(soup_obj)\n\u001b[0;32m     10\u001b[0m ratings_list \u001b[39m=\u001b[39m get_stars(soup_obj)\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mload_soup_object\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_soup_object\u001b[39m(url):\n\u001b[0;32m      2\u001b[0m     \u001b[39m###\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[39m#url = \"https://www.thespruceeats.com/search?q=&searchType=recipe\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[0;32m      5\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m soup\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:573\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[39m# Create the Request.\u001b[39;00m\n\u001b[0;32m    561\u001b[0m req \u001b[39m=\u001b[39m Request(\n\u001b[0;32m    562\u001b[0m     method\u001b[39m=\u001b[39mmethod\u001b[39m.\u001b[39mupper(),\n\u001b[0;32m    563\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    571\u001b[0m     hooks\u001b[39m=\u001b[39mhooks,\n\u001b[0;32m    572\u001b[0m )\n\u001b[1;32m--> 573\u001b[0m prep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_request(req)\n\u001b[0;32m    575\u001b[0m proxies \u001b[39m=\u001b[39m proxies \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    577\u001b[0m settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge_environment_settings(\n\u001b[0;32m    578\u001b[0m     prep\u001b[39m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[0;32m    579\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    481\u001b[0m     auth \u001b[39m=\u001b[39m get_netrc_auth(request\u001b[39m.\u001b[39murl)\n\u001b[0;32m    483\u001b[0m p \u001b[39m=\u001b[39m PreparedRequest()\n\u001b[1;32m--> 484\u001b[0m p\u001b[39m.\u001b[39;49mprepare(\n\u001b[0;32m    485\u001b[0m     method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(),\n\u001b[0;32m    486\u001b[0m     url\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl,\n\u001b[0;32m    487\u001b[0m     files\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mfiles,\n\u001b[0;32m    488\u001b[0m     data\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m    489\u001b[0m     json\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mjson,\n\u001b[0;32m    490\u001b[0m     headers\u001b[39m=\u001b[39;49mmerge_setting(\n\u001b[0;32m    491\u001b[0m         request\u001b[39m.\u001b[39;49mheaders, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders, dict_class\u001b[39m=\u001b[39;49mCaseInsensitiveDict\n\u001b[0;32m    492\u001b[0m     ),\n\u001b[0;32m    493\u001b[0m     params\u001b[39m=\u001b[39;49mmerge_setting(request\u001b[39m.\u001b[39;49mparams, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams),\n\u001b[0;32m    494\u001b[0m     auth\u001b[39m=\u001b[39;49mmerge_setting(auth, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth),\n\u001b[0;32m    495\u001b[0m     cookies\u001b[39m=\u001b[39;49mmerged_cookies,\n\u001b[0;32m    496\u001b[0m     hooks\u001b[39m=\u001b[39;49mmerge_hooks(request\u001b[39m.\u001b[39;49mhooks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhooks),\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    498\u001b[0m \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_method(method)\n\u001b[1;32m--> 368\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_url(url, params)\n\u001b[0;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_headers(headers)\n\u001b[0;32m    370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[1;32mc:\\Users\\beta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:439\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39m*\u001b[39me\u001b[39m.\u001b[39margs)\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m scheme:\n\u001b[1;32m--> 439\u001b[0m     \u001b[39mraise\u001b[39;00m MissingSchema(\n\u001b[0;32m    440\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No scheme supplied. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerhaps you meant http://\u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m host:\n\u001b[0;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No host supplied\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL 'Recipe_link': No scheme supplied. Perhaps you meant http://Recipe_link?"
     ]
    }
   ],
   "source": [
    "df = fast_scrape('Recipe_Links_and_Names.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataframe lets see what we are working with. We will try to visualize the data and clean it up later of course we want some functions :)  We will make functions for scatter plot, \n",
    "histogram and a 1 column pie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter_2_params(col_name_1,col_name_2):\n",
    "    df=pd.read_csv('my_data.csv')\n",
    "    df['Fat'] = df['Fat'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Carbs'] = df['Carbs'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Protein'] = df['Protein'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df.plot.scatter(x=col_name_1, y=col_name_2)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histo_1_params(col_name):\n",
    "    # read in your dataframe from a csv file\n",
    "    df = pd.read_csv('my_data.csv')\n",
    "    df['Fat'] = df['Fat'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Carbs'] = df['Carbs'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Protein'] = df['Protein'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    # choose the column you want to use for the histogram\n",
    "\n",
    "    # sort the column values into bins\n",
    "    bin_values, bin_edges = np.histogram(df[col_name], bins='auto')\n",
    "\n",
    "    # create the histogram using the sorted bins\n",
    "    plt.hist(df[col_name], bins=bin_edges)\n",
    "\n",
    "    # add labels and title to the histogram\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of ' + col_name)\n",
    "\n",
    "    # display the histogram\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pie_1_params(col_name):\n",
    "    df = pd.read_csv('my_data.csv')\n",
    "    df['Fat'] = df['Fat'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Carbs'] = df['Carbs'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df['Protein'] = df['Protein'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    # choose the column you want to use for the pie chart    \n",
    "\n",
    "    # get the count of unique values in the column\n",
    "    value_counts = df[col_name].value_counts()\n",
    "\n",
    "    # create the pie chart\n",
    "    plt.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')\n",
    "\n",
    "    # add title to the pie chart\n",
    "    plt.title('Pie Chart of ' + col_name)\n",
    "\n",
    "    # display the pie chart\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in addition we wanted a pie chart of how much is meat dairy meat and dairy and fur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pie_meat_dairy_fur():\n",
    "    df = pd.read_csv('my_data.csv')\n",
    "    \n",
    "    # Calculate the number of recipes in each category\n",
    "    meat_count = len(df[df['Meat'] == 1])\n",
    "    dairy_count = len(df[df['Dairy'] == 1])\n",
    "    fur_count = len(df[df['Fur'] == 1])\n",
    "    dairy_meat_count = len(df[(df['Dairy'] == 1) & (df['Meat'] == 1)])\n",
    "\n",
    "    # Create a list of category counts and labels\n",
    "    counts = [meat_count, dairy_count, fur_count, dairy_meat_count]\n",
    "    labels = ['Meat', 'Dairy', 'Fur', 'Dairy&Meat']\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.pie(counts, labels=labels, autopct='%1.1f%%')\n",
    "\n",
    "    # Add a title to the chart\n",
    "    plt.title('Recipe Categories')\n",
    "\n",
    "    # Show the chart\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets run all of these and see if we can already draw some conclusions here come some more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_all_histo(df):\n",
    "    #df=scraping_functions.clean_df('my_data.csv')\n",
    "    # scraping_functions.save_df(df,'clean.csv')\n",
    "    # df= pd.read_csv('clean.csv')\n",
    "    numeric_cols = df.select_dtypes(include=['int', 'float']).columns.to_list()\n",
    "    # print(numeric_cols)\n",
    "    # numeric_cols.remove(\"Unnamed: 0\")\n",
    "    # print(numeric_cols)\n",
    "    #print(numeric_cols)\n",
    "    # # loop over the selected columns\n",
    "    print(df.describe())\n",
    "    for col in numeric_cols:\n",
    "        draw_histo_1_params(df,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_all_histo(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see in some of those graphs(we need to add their names here)we have outliers. We need to clean up the data instead of going over each column and manually cleaning it we will clean the whole dataframe at once using the IQR methoed and some carefull tinkering and some columns have unnecesery letters that if we delete they turn into numeric columns(fat,carbs ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df,column_name):\n",
    "    \n",
    "    print(df[column_name].describe())\n",
    "    # Calculate the IQR of the Prep column\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Determine the upper and lower bounds for the Prep column\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 10 * IQR\n",
    "\n",
    "    # Replace any values outside of the bounds with NaN\n",
    "    df.loc[(df[column_name] < lower_bound) | (df[column_name] > upper_bound), column_name] = float('NaN')\n",
    "    #df = df.dropna(subset=[column_name])\n",
    "    df.dropna(inplace=True)\n",
    "    print(df[column_name].describe())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_g_fron_col(df,col_name):\n",
    "    df[col_name] = df[col_name].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    #df=pd.read_csv(filename)\n",
    "    #df=clean_data_time(df)\n",
    "    df=remove_g_fron_col(df,'Fat')\n",
    "    df=remove_g_fron_col(df,'Carbs')\n",
    "    df=remove_g_fron_col(df,'Protein')\n",
    "\n",
    "    \n",
    "    #print(df)\n",
    "    numeric_cols = df.select_dtypes(include=['int', 'float']).columns\n",
    "    #print(numeric_cols)\n",
    "    # # loop over the selected columns\n",
    "    for col in numeric_cols:\n",
    "        clean(df,col)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also needed to fix some errors with the time parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_time(df):\n",
    "    #df = pd.read_csv('clean_df.csv')\n",
    "\n",
    "    # Replace Prep, Cook, and Total values with median if Prep + Cook != Total\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Prep'] + row['Cook'] != row['Total']:\n",
    "            if row['Prep'] != 0:\n",
    "                df.at[index, 'Cook'] = df.at[index, 'Total'] - df.at[index, 'Prep']\n",
    "            elif row['Cook'] != 0:\n",
    "                df.at[index, 'Prep'] = df.at[index, 'Total'] - df.at[index, 'Cook']\n",
    "            else:\n",
    "                df.drop(index, inplace=True)\n",
    "\n",
    "    # Fill missing Total values with Prep + Cook\n",
    "    df['Total'].fillna(df['Prep'] + df['Cook'], inplace=True)\n",
    "\n",
    "    # Fill missing Prep or Cook values if only one is missing\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row['Prep']):\n",
    "            if pd.notna(row['Cook']) and pd.notna(row['Total']):\n",
    "                df.at[index, 'Prep'] = df.at[index, 'Total'] - df.at[index, 'Cook']\n",
    "            else:\n",
    "                df.drop(index, inplace=True)\n",
    "        elif pd.isna(row['Cook']):\n",
    "            if pd.notna(row['Prep']) and pd.notna(row['Total']):\n",
    "                df.at[index, 'Cook'] = df.at[index, 'Total'] - df.at[index, 'Prep']\n",
    "            else:\n",
    "                df.drop(index, inplace=True)\n",
    "        elif pd.isna(row['Total']):\n",
    "            if pd.notna(row['Prep']) and pd.notna(row['Cook']):\n",
    "                df.at[index, 'Total'] = df.at[index, 'Prep'] + df.at[index, 'Cook']\n",
    "            else:\n",
    "                df.drop(index, inplace=True)\n",
    "\n",
    "    # Save cleaned dataframe to new csv file\n",
    "    #df = df.drop(columns='Unnamed')\n",
    "    #df = df.drop(df.columns[0], axis=1)\n",
    "    #print(df.columns)\n",
    "    #df.to_csv('Cleaned_df_updated.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data_time(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets review the results and ensure we are ok with them (shpira plz fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_all_histo(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok looks good now lets try and visualize the data some more to try and \"get a feel\" for our data(shapira plz fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pie_meat_dairy_fur(df):\n",
    "\n",
    "    #df = pd.read_csv('my_data.csv')\n",
    "    \n",
    "    # Calculate the number of recipes in each category\n",
    "    meat_count = len(df[df['Meat'] == 1])\n",
    "    dairy_count = len(df[df['Dairy'] == 1])\n",
    "    fur_count = len(df[df['Fur'] == 1])\n",
    "    dairy_meat_count = len(df[(df['Dairy'] == 1) & (df['Meat'] == 1)])\n",
    "\n",
    "    # Create a list of category counts and labels\n",
    "    counts = [meat_count, dairy_count, fur_count, dairy_meat_count]\n",
    "    labels = ['Meat', 'Dairy', 'Fur', 'Dairy&Meat']\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.pie(counts, labels=labels, autopct='%1.1f%%')\n",
    "\n",
    "    # Add a title to the chart\n",
    "    plt.title('Recipe Categories')\n",
    "\n",
    "    # Show the chart\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter_2_params(df,col_name_1,col_name_2):    \n",
    "    # df['Fat'] = df['Fat'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    # df['Carbs'] = df['Carbs'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    # df['Protein'] = df['Protein'].str.replace('g', '').str.replace(',', '').astype(int)\n",
    "    df.plot.scatter(x=col_name_1, y=col_name_2)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we went over every possible option to draw a scatter plot and we found it hard to draw to correlations this way so we went with a mathematical aproach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(df, col1, col2):\n",
    "    return df[col1].corr(df[col2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we ran this with each and every column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import correlation\n",
    "\n",
    "\n",
    "def get_corr_all_columns(df):\n",
    "    df = pd.read_csv('clean_modified.csv')\n",
    "    numeric_cols = df.select_dtypes(include=['int', 'float'])\n",
    "    if 'Unnamed' in numeric_cols.columns:\n",
    "        numeric_cols = numeric_cols.drop('Unnamed', axis=1)\n",
    "\n",
    "    correlation = {}\n",
    "    for col1 in numeric_cols:\n",
    "        for col2 in numeric_cols:\n",
    "            if col1 != col2:\n",
    "                corr_value = get_correlation(df, col1, col2)\n",
    "                if corr_value > 0.5 or corr_value < -0.5:\n",
    "                    correlation[(col1, col2)] = corr_value\n",
    "\n",
    "    # Sort the correlations by their absolute value, in descending order\n",
    "    sorted_correlations = sorted(correlation.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "    # Print the correlations above 0.5 or below -0.5\n",
    "    for corr, value in sorted_correlations:\n",
    "        if abs(value) > 0.5:\n",
    "            print(f\"{corr[0]} and {corr[1]}: {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sadly no correlation with rating "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our idea from here was to try and figure What factors contribute to the popularity of a recipe, and how accurately can we predict a recipe's popularity based on these factors? to that end we wanted to filter our ingridients as we assumed that particular ingridients could possitively or negetively effect the rating.\n",
    "let the cleaning beggin!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see what we are working with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_data_review(df):\n",
    "    #Create a list of all the ingredients\n",
    "    #df=pd.read_csv('my_data.csv')\n",
    "    all_ingredients = []\n",
    "\n",
    "    for i in df['Ingredients']:\n",
    "        all_ingredients += eval(i)\n",
    "    # Define a list of terms to exclude from the ingredients list\n",
    "    exclude_terms = ['salt', 'pepper', 'garlic', 'onion', 'paprika', 'cumin', 'chili', 'oregano', 'basil', 'thyme', 'rosemary']\n",
    "    # Create a list of all the ingredients\n",
    "    clean_ingredients = []\n",
    "    for ingredient in all_ingredients:\n",
    "        ingredient = re.sub(r'\\d+(\\.\\d+)?', '', ingredient) # Remove any quantity\n",
    "        ingredient = re.sub(r'(\\s+\\d+)?\\s*(large|medium|small)?\\s*(cup|teaspoon|tablespoon)s?', '', ingredient, flags=re.IGNORECASE) # Remove any volume/capacity description\n",
    "        ingredient = ingredient.strip()\n",
    "        if ingredient and not any(term in ingredient.lower() for term in exclude_terms):\n",
    "            clean_ingredients.append(ingredient)\n",
    "\n",
    "    # Count the frequency of each ingredient\n",
    "    ingredient_counts = pd.Series(clean_ingredients).value_counts()\n",
    "\n",
    "    # Create a pie chart for the top 10 ingredients\n",
    "    top_10_ingredients = ingredient_counts.head(10)\n",
    "    plt.pie(top_10_ingredients, labels=top_10_ingredients.index, autopct='%1.1f%%')\n",
    "    plt.title('Top 10 Ingredients in Recipes (Excluding Spices)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data_review(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see the results dont realy teach us anything but we wanted to clean them because they come with data such as the volume or weight or special characters and that isn't what we need also the names differ when talking about the same thing like special salts they are all just salt it also results in some duplicates in some cases we need to fix this  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our idea was to make a dictionary of the top X ingridients and manully filter the junk out of them first lets make the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def find_most_common_ingridients(df):\n",
    "    final_lst = []\n",
    "    #df = pd.read_csv('clean.csv')\n",
    "    ingredients = df['Ingredients'].tolist()\n",
    "\n",
    "    for item in ingredients:\n",
    "        temp = item.replace(\"'\", \"\")   \n",
    "        temp = item.replace(\"\\\\\\\\xa0\", \" \")\n",
    "        temp = item.replace(\"\\\\\\\\u200b\", \" \")\n",
    "        temp = temp.strip(\"[]\").split(\", \")\n",
    "        temp_tokens = []\n",
    "        for item in temp:\n",
    "            temp_tokens.extend(item.split())\n",
    "        final_lst.extend(temp_tokens)\n",
    "\n",
    "    #print(final_lst)\n",
    "    word_count = collections.Counter(final_lst)\n",
    "\n",
    "    # Filter out items that start with a digit\n",
    "    sorted_dict = {k: v for k, v in word_count.items() if not k[0].isdigit()}\n",
    "    sorted_dict = dict(sorted(sorted_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    print('\\n\\n\\n\\n')\n",
    "    # Print the top 100 items\n",
    "\n",
    "    # Create a DataFrame from the sorted dictionary and save it as a CSV file\n",
    "    df = pd.DataFrame.from_dict(sorted_dict, orient='index', columns=['count'])\n",
    "    df.to_csv('Ingredients_dirty.csv')\n",
    "    \n",
    "    for i, (key, value) in enumerate(sorted_dict.items()):\n",
    "        if i == 300:\n",
    "            break\n",
    "        print(f\"{key}: {value}\")\n",
    "    return sorted_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now with the power of notepad++ and some regex expressions we first made everything lower case delete any cell with numbers in it removed duplicates and clustered when it made sense "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(figure out how to upload a csv with the notebook we need to upload the ingridients)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have what our *relevant* ingridients should look like time to filter them out if an ingridient apears as a substring in the original ingridients swap it to what is in the new csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ingridients():\n",
    "    with open('ingridients.csv', 'r') as file:\n",
    "    # Initialize an empty list to store the data\n",
    "        data = []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            values = line.split(',')\n",
    "            return values\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_index_columns(df):\n",
    "    # Get a list of all column names that start with 'Unnamed'\n",
    "    index_cols = [col for col in df.columns if col.startswith('Unnamed')]\n",
    "    \n",
    "    # Drop the columns from the dataframe\n",
    "    df.drop(columns=index_cols, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ingridients(df):\n",
    "    df=delete_index_columns(df)\n",
    "    df2=df.copy()\n",
    "    pure_ingredients=read_ingridients()\n",
    "    print(\"starting ingridient cleanup this might take a minute\")\n",
    "    #df=pd.read_csv(filename)\n",
    "    for i, ingredient in enumerate(df2['Ingredients']):\n",
    "        line = ingredient.split(\",\")\n",
    "        for j, item in enumerate(line):\n",
    "            for single_ingredient in pure_ingredients:\n",
    "                if single_ingredient.lower() in item.lower():\n",
    "                    #print(f'found {single_ingredient} in {item}')\n",
    "                    line[j] = single_ingredient.lower()\n",
    "        df2.loc[i, 'Ingredients'] = \",\".join(line)\n",
    "        #print(line)\n",
    "        #print('\\n\\n')\n",
    "    #print(df['Ingredients'])\n",
    "    # save the modified dataframe to a new CSV file\n",
    "    df2=delete_index_columns(df)\n",
    "    #df2.to_csv('clean_modified.csv', index=False)\n",
    "    print(\"done!\")\n",
    "   \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_ingridients(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets visualise this and hope that we cleaned it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ingridients_dict(df):\n",
    "    ingredients = []\n",
    "\n",
    "    with open('ingridients.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            ingredients.extend(row)\n",
    "\n",
    "    #df= pd.read_csv('clean_modified.csv')\n",
    "\n",
    "\n",
    "    ingredients_dict = {}\n",
    "\n",
    "    # loop through the ingredients in the DataFrame\n",
    "    for line in df['Ingredients']:\n",
    "        for single_item in line.split(','): # split the ingredients by comma if they're in a single string\n",
    "            for ing_in_lst in ingredients: # loop through the ingredients in the ingredients list\n",
    "                if ing_in_lst in single_item.strip(): # check if the ingredient in the list is a substring of the ingredient in the dataset\n",
    "                    # if the ingredient exists in the dictionary, increment its count\n",
    "                    if ing_in_lst in ingredients_dict:\n",
    "                        ingredients_dict[ing_in_lst] += 1\n",
    "                    # otherwise, add the ingredient to the dictionary with a count of 1\n",
    "                    else:\n",
    "                        ingredients_dict[ing_in_lst] = 1\n",
    "\n",
    "    # sort the dictionary by value in descending order\n",
    "    sorted_dict = dict(sorted(ingredients_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # print the dictionary of ingredients and their counts\n",
    "    # print(\"Ingredient Counts:\")\n",
    "    # for ing, count in sorted_dict.items():\n",
    "    #     print(f'{ing}: {count}')   \n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ingridient_pie_chart(top_n):\n",
    "    # print()\n",
    "    ingredient_counts = get_ingridients_dict()\n",
    "\n",
    "    # Extract the top N ingredients by frequency\n",
    "    ingredient_names = collections.Counter(ingredient_counts).most_common(top_n)\n",
    "    ingredient_names = [x[0] for x in ingredient_names]\n",
    "    ingredient_frequencies = [ingredient_counts[name] for name in ingredient_names]\n",
    "    total_receipe_count = sum(ingredient_frequencies)\n",
    "    ingredient_percentages = [(count/total_receipe_count)*100 for count in ingredient_frequencies]\n",
    "    ingredient_labels = ['{} ({:.1f}%)'.format(name, percentage) for name, percentage in zip(ingredient_names, ingredient_percentages)]\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.pie(ingredient_percentages, labels=ingredient_labels)\n",
    "    plt.title('Top {} Ingredients'.format(top_n))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like it works properly :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we want to see if there is a correlation between the ingridients and the rating to achive this we \"exploded\" our ingridients into binary columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_ingridients_and_get_corr(df):#this function expands the dataframe to each ingridient and tries to find a correlation\n",
    "    #df = pd.read_csv('clean_modified.csv')\n",
    "\n",
    "    # extract the ingredients column\n",
    "    df_ing = df['Ingredients']\n",
    "\n",
    "    # define a list of ingredients you care about\n",
    "    important_ingredients = []\n",
    "\n",
    "    with open('ingridients.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            important_ingredients.extend(row)\n",
    "\n",
    "    # loop through the ingredients you care about\n",
    "    for ingredient in important_ingredients:\n",
    "        # create a new column with a value of 1 if the recipe contains the ingredient and 0 otherwise\n",
    "        df[ingredient] = df_ing.str.contains(ingredient).astype(int)\n",
    "\n",
    "    correlations = df[['Rating'] + [ingredient for ingredient in important_ingredients]].corr()\n",
    "\n",
    "    # print the correlation coefficients for each ingredient\n",
    "    # print(correlations.loc['Rating'])\n",
    "    top_20 = correlations.loc['Rating'].sort_values(ascending=False)[1:21]\n",
    "    print(\"Top 20 ingredients with the highest correlation with the 'Rating' column:\")\n",
    "    print(top_20)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= explode_ingridients_and_get_corr(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(fancy english word ) we found no correlations with ingridients and the rating (liquer was funny)\n",
    "so that answears one of our research questions beucase nothing gave us a correlation with rating we consider that there isnt anything that can *directly* predict the rating of a recepie to further demonstrate this we decided to draw a heatmap and show that there is nothing special in it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the natural next step is to try machine learning and see if it can predict the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(get linear regression from shapira)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our accuracy hovers around 0.36(to confirm) which isnt that good so we played around with feature engineering to try and improve our predictions we decide to make a popularity colum that would sum the frequncy of each ingridient in the recepie also we decided to make a difficulty column that takes into acount the steps cooking time etc...(shapira plz fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_popularity_score_to_df(df):\n",
    "    binary_columns = [col for col in df.columns if col not in ['Name', 'Prep', 'Cook', 'Total', 'Servings', 'Rating', 'Rating_Count', 'Dairy', 'Meat', 'Fur', 'Calories', 'Fat', 'Carbs', 'Protein', 'Num_ing', 'Steps', 'Ingredients']]\n",
    "\n",
    "    # Calculate the frequencies of each binary value\n",
    "    ingredient_frequencies = df[binary_columns].sum() / len(df) * 30\n",
    "\n",
    "    # Calculate the popularity score for each recipe\n",
    "    popularity_scores = []\n",
    "    for _, row in df.iterrows():\n",
    "        score = 0\n",
    "        for column in binary_columns:\n",
    "            if row[column] == 1:\n",
    "                score += ingredient_frequencies[column]\n",
    "        popularity_scores.append(score)\n",
    "\n",
    "    # Add the popularity scores as a new column to the DataFrame\n",
    "    df['Popularity Score'] = popularity_scores\n",
    "\n",
    "    # Print the updated DataFrame\n",
    "    #print(df)\n",
    "    #scraping_functions.draw_histo_1_params(df,'Popularity Score')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_difficulty_column(df):\n",
    "    # Read the CSV file into a pandas dataframe\n",
    "    #df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Calculate the combined score for each recipe\n",
    "    df['Combined'] = df['Num_ing'] + df['Steps'] + df['Total']\n",
    "    \n",
    "    # Calculate the mean and standard deviation of the combined score\n",
    "    mean = df['Combined'].mean()\n",
    "    std_dev = df['Combined'].std()\n",
    "    \n",
    "    # Calculate the z-score for each recipe\n",
    "    df['Z_score'] = (df['Combined'] - mean) / std_dev\n",
    "    \n",
    "    # Divide the z-scores into 5 equal-sized groups\n",
    "    df['Difficulty'] = pd.qcut(df['Z_score'], q=5, labels=[1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Write the updated dataframe to a new CSV file\n",
    "    # new_csv_file_path = csv_file_path.split('.csv')[0] + '_with_difficulty.csv'\n",
    "    # df.to_csv(new_csv_file_path, index=False)\n",
    "    #df.to_csv(csv_file_path, index=False, mode='w')\n",
    "    #save_df(df,\"csv_Wtih_diff.csv\")\n",
    "    \n",
    "    # Print the number of recipes in each difficulty level\n",
    "    for i in range(1, 6):\n",
    "        count = df['Difficulty'][df['Difficulty'] == i].count()\n",
    "        print(f\"Difficulty level {i}: {count}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets re run our linear regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_rating_linear(df) ## shapira i need your code boiiiii"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "amazingly we got a score of 0.54 which is bad but it is a 40% improvement over our previous attempt\n",
    "obviously we didnt include columns that and feature engineering attempts that didnt have significant results \n",
    "among the ideas we tried (insert bullshit from chatgpt here shapira plz fix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seeing as our result is 0.54 we decided to change tactics a little even though our question is Is it possible to determine if a recipe will receive a high rating? we dont have to predict it stars and go from there"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead what we decided is that a recepie which is rated for 5 stars is agruably a good recepie (we have debates where can you draw the line maybe 4.5 is also good enough but ulimately 5 stars is supposed to be the best of the best so we went with that)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we decided to eventually use a model called perseptron which is a binary classification model meaning its trained to figure out if a recepie is 5 stars or not( we were advised to go with this aproach by our instructor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def build_perceptron_model(df):\n",
    "    #df = pd.read_csv('test123.csv')\n",
    "\n",
    "    # Save the 'Rating' column separately\n",
    "    ratings = df['Rating']\n",
    "    \n",
    "    # Drop the 'Rating' column from the dataframe\n",
    "    df = df.drop(['Rating'], axis=1)\n",
    "    df = df.drop(['Rating_Count'], axis=1)\n",
    "    df = df.select_dtypes(include=['int', 'float'])\n",
    "    # Get the column names of the dataframe\n",
    "    col_names = df.columns\n",
    "\n",
    "    # Convert the dataframe to a numpy array\n",
    "    X = df.select_dtypes(include=['int', 'float']).values\n",
    "\n",
    "    # Define the target variable\n",
    "    y = (ratings > 4.5).astype(int).values\n",
    "\n",
    "    # Verify that X and y have the same length\n",
    "    assert len(X) == len(y), \"X and y must have the same length\"\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the Perceptron model to the training data\n",
    "    model = Perceptron()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Compute the predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Get the weights learned by the perceptron\n",
    "    weights = model.coef_[0]\n",
    "\n",
    "    # Create a new dataframe with the column names and weights\n",
    "    weights_df = pd.DataFrame({'column': col_names, 'weight': weights})\n",
    "\n",
    "    # Print the weights dataframe\n",
    "    print(weights_df)\n",
    "\n",
    "    #save_df(weights_df,'FML.csv')\n",
    "    return weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_perceptron_model(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we got an amazing result of 0.95 which is good too good to be true so we tested on 20 recepies from another site (recepies that didnt apear on our site ) and it accurately predicted if they were 5 star recepies or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by='weight')\n",
    "\n",
    "# printing the top 10 rows\n",
    "print(sorted_df.tail(10)[['column', 'weight']])\n",
    "\n",
    "# printing the bottom 10 rows\n",
    "print(sorted_df.head(10)[['column', 'weight']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also printed the weights of the recepie the top 5 most contributing factors to a recepie being a 5 star recepie are (fix? )\n",
    "Prep time\n",
    "Cook time\n",
    "Servings\n",
    "Calories\n",
    "Number of ingredients\n",
    "Number of steps\n",
    "as their weights were the highest in our weights dataframe also the 5 worst ingridients were\n",
    "Popularity Score with a weight of -5960.882178301813\n",
    "Prep with a weight of -2079.0\n",
    "Cook with a weight of -5674.0\n",
    "Total with a weight of -7753.0\n",
    "Servings with a weight of -1836.0\n",
    "\n",
    "these are high number that help define recepies that are not! 5* from this we can understand that people liked the recepies with shorter cooking time be it prep the cooking itself or the mix , low amount of servings(meaning high calories because of our previous results) and unique ingridients because the popularity score is highly negetive\n",
    "which supports our idea of adding rare ingridients (caviar,lobster,etc) can improve the odds of making a recepie a high rating one :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to conclude : bla bla bla shapira pls help :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize results? haha updated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
